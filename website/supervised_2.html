<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Supervised Learning - ML Project</title>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/aos/2.3.4/aos.css" rel="stylesheet">
    <link href="css/styles.css" rel="stylesheet">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
    <style>
        /* Dropdown styles */
        .dropdown {
            position: relative;
            display: inline-block;
        }
        
        .dropdown-content {
            display: none;
            position: absolute;
            top: 100%;
            margin-top: 10px;
            background: rgba(0, 0, 0, 0.85);
            min-width: 160px;
            box-shadow: 0 8px 16px rgba(0, 0, 0, 0.3);
            z-index: 1;
            border-radius: 0.5rem;
            backdrop-filter: blur(10px);
        }
        
        .dropdown-content a {
            color: white;
            padding: 12px 16px;
            text-decoration: none;
            display: block;
            transition: background 0.3s ease;
        }
        
        .dropdown-content a:hover {
            background: var(--primary);
        }
        
        /* Add padding to create hover area */
        .dropdown-content::before {
            content: '';
            position: absolute;
            top: -10px;
            left: 0;
            width: 100%;
            height: 10px;
        }
        
        .dropdown:hover .dropdown-content {
            display: block;
        }
        
        .content-section {
            background: rgba(255, 255, 255, 0.05);
            padding: 3rem;
            border-radius: 1rem;
            margin: 2rem 0;
            backdrop-filter: blur(10px);
            border: 1px solid rgba(255, 255, 255, 0.1);
        }

        .visualization-section {
            margin: 4rem 0;
            padding: 2rem;
            background: rgba(255, 255, 255, 0.02);
            border-radius: 1rem;
            border: 1px solid rgba(255, 255, 255, 0.05);
        }
        
        .visualization-section img{
            transition: transform 0.3s ease;
        }

        .visualization-section img:hover{
            transform: scale(1.025);
        }

        .select-container {
            margin: 2rem 0;
        }

        select {
            background: var(--accent);
            color: white;
            padding: 0.8rem 2rem;
            border: none;
            border-radius: 0.5rem;
            font-size: 1.2rem;
            cursor: pointer;
            transition: transform 0.3s ease;
        }

        select:hover {
            transform: scale(1.05);
        }

        .plot-container {
            width: 100%; 
            text-align: center;
            max-width: 1000px;
            margin: 2rem auto;
            border-radius: 1rem;
            overflow: hidden;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.3);
            transition: transform 0.3s ease;
            background: rgba(0, 0, 0, 0.2);
            padding: 1rem;
        }

        .ul-list {
            font-size: 1.5rem;
            max-width: 800px;
            margin: 1rem auto;
            text-align: justify;
        }
        .plot-container img {
            width: 90%;
            height: auto;
            display: block;
            margin: 0 auto;
            border-radius: 0.5rem;
        }

        .plot-description {
            margin: 2rem 0;
            padding: 2rem;
            background: rgba(255, 255, 255, 0.03);
            border-radius: 1rem;
            border: 1px solid rgba(255, 255, 255, 0.05);
        }
        
        h2 {
            color: var(--secondary);
            margin: 3rem 0 1.5rem 0;
            font-size: 2.5rem;
        }

        h3 {
            color: var(--primary);
            margin: 2rem 0 1rem 0;
            font-size: 2rem;
        }

        .navigation-buttons {
            display: flex;
            justify-content: space-between;
            margin: 3rem 0;
        }

        .nav-button {
            padding: 1rem 2rem;
            background: var(--accent);
            border: none;
            border-radius: 1rem;
            color: white;
            cursor: pointer;
            font-size: 1.2rem;
            transition: transform 0.3s ease;
        }

        .nav-button:hover {
            transform: scale(1.1);
        }

        .text-centered {
            text-align: center;
        }

        .text-justified {
            text-align: justify;
        }

        a {
            color: var(--primary);
        }

        .result-table {
            width: 100%;
            margin: 2rem 0;
            border-collapse: collapse;
            background: rgba(255, 255, 255, 0.03);
            border-radius: 0.5rem;
            overflow: hidden;
        }

        .result-table th, .result-table td {
            padding: 1rem;
            text-align: left;
            border-bottom: 1px solid rgba(255, 255, 255, 0.1);
        }

        .result-table th {
            background: rgba(255, 255, 255, 0.05);
            font-weight: bold;
        }
        
        .two-column-container {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 2rem;
            margin: 2rem 0;
        }
        
        .method-card {
            background: rgba(255, 255, 255, 0.03);
            padding: 2rem;
            border-radius: 1rem;
            border: 1px solid rgba(255, 255, 255, 0.05);
            transition: transform 0.3s ease;
        }
        
        .method-card:hover {
            transform: scale(1.02);
        }

        .code-block {
            background: rgba(0, 0, 0, 0.3);
            padding: 1.5rem;
            border-radius: 0.5rem;
            overflow-x: auto;
            font-family: monospace;
            margin: 1.5rem 0;
            border-left: 3px solid var(--primary);
        }

        .comparison-table {
            width: 100%;
            margin: 2rem 0;
            border-collapse: collapse;
            border-radius: 0.5rem;
            overflow: hidden;
        }

        .comparison-table th, .comparison-table td {
            padding: 1rem;
            text-align: left;
            border: 1px solid rgba(255, 255, 255, 0.1);
        }

        .comparison-table th {
            background: rgba(255, 255, 255, 0.1);
            color: var(--secondary);
        }

        .comparison-table tr:nth-child(even) {
            background: rgba(255, 255, 255, 0.03);
        }

        .comparison-table table {
            width: 100%;
        }

        .equation {
            /* background: rgba(255, 255, 255, 0.05); */
            padding: 1rem;
            border-radius: 0.5rem;
            margin: 1rem 0;
            text-align: center;
            font-family: monospace;
            font-size: 1.2rem;
            color: var(--primary);
        }

    </style>
</head>
<body>
    <nav>
        <ul>
            <li><a href="index.html">Home</a></li>
            <li><a href="introduction.html">Introduction</a></li>
            <li><a href="data-exploration.html">Data Preparation & Exploration</a></li>
            <li class="dropdown">
                <a href="unsupervised.html">Unsupervised Learning</a>
                <div class="dropdown-content">
                    <a href="unsupervised.html#pca">PCA</a>
                    <a href="unsupervised.html#clustering">Clustering</a>
                    <a href="unsupervised.html#arm">ARM</a>
                </div>
            </li>
            <li class="dropdown">
                <a href="supervised.html" class="active">Supervised Learning</a>
                <div class="dropdown-content">
                    <a href="supervised.html#naive-bayes">Naive Bayes</a>
                    <a href="supervised.html#decision-trees">Decision Trees</a>
                    <a href="supervised.html#regression">Regression</a>
                    <a href="supervised_2.html#svm">SVM</a>
                    <a href="supervised_2.html#ensemble">Ensemble</a>
                </div>
            </li>
            <li><a href="conclusion.html">Conclusion</a></li>
            <li><a href="about.html">About Me</a></li>
        </ul>
    </nav>

    <div class="content-container">
        <div class="content-section" data-aos="fade-up">
            <br><br><br>
            <h1>Supervised Learning (cont.)</h1>
            <!-- <p class="text-justified">
                Supervised learning is a cornerstone of machine learning where models learn from labeled training data to make predictions on unseen data. This section explores four powerful supervised techniques applied to the audio dataset: Naive Bayes for probabilistic classification, Decision Trees for rule-based decisions, Regression for predicting continuous values, and Support Vector Machines (SVM) for finding optimal decision boundaries. Each method offers unique strengths for audio classification tasks.
            </p> -->
        </div>

        <!-- Support Vector Machines Section -->
        <div id="svm" class="content-section" data-aos="fade-up">
            <h2>Support Vector Machines</h2>
            
            <h3 class="text-centered">Overview</h3>
            <p class="text-justified">
                Support Vector Machines (SVMs) are powerful supervised learning algorithms used for classification, regression, and outlier detection. SVMs are particularly effective in high-dimensional spaces and cases where the number of dimensions exceeds the number of samples. The core idea behind SVMs is to find the optimal hyperplane that maximizes the margin between different classes in the feature space. This margin maximization gives SVMs strong generalization properties, making them less prone to overfitting compared to many other classifiers.
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/svm.png" alt="SVM Concept">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This illustration shows the fundamental concept of SVMs. The algorithm finds the hyperplane that maximizes the margin between classes. The margin is the distance between the hyperplane and the closest data points from each class, known as support vectors (circled). These support vectors are the only data points that determine the hyperplane's position, making SVMs robust to outliers far from the decision boundary.
                    </p>
                </div>
            </div>


            <h3 class="text-centered">Linear Separability and Hyperplanes</h3>
            <p class="text-justified">
                At their core, SVMs are linear classifiers that separate data points using a hyperplane. In a 2D space, this hyperplane is a straight line; in 3D, it's a flat plane; and in higher dimensions, it becomes an n-1 dimensional flat subspace. The decision function for an SVM is based on the position of a data point relative to this hyperplane, which is defined by the equation:
            </p>
            
            <div class="equation">
                f(x) = w · x + b
            </div>
            
            <p class="text-justified">
                Where w is the normal vector to the hyperplane, x is the input feature vector, and b is the bias term. The classification rule is simple: if f(x) ≥ 0, the point belongs to the positive class; otherwise, it belongs to the negative class.
            </p>
            
            <p class="text-justified">
                What makes SVMs special is how they choose the optimal hyperplane among infinitely many possible separating hyperplanes. The SVM algorithm selects the hyperplane that maximizes the margin—the distance between the hyperplane and the closest data points from each class, called support vectors. This maximum margin strategy enhances the model's generalization ability to unseen data.
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/svm_margin.png" alt="SVM Margin Concept">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This image illustrates the concept of margin maximization in SVMs. The left panel shows multiple possible hyperplanes (green lines) that can separate the two classes (red squares and blue circles). The right panel highlights the optimal hyperplane (solid green line) which maximizes the margin between the classes. The margin boundaries (dashed green lines) pass through the support vectors (filled red squares and blue circle), which are the critical data points that define the decision boundary. By maximizing this margin, SVMs create more robust classification boundaries that generalize better to unseen data.
                    </p>
                </div>
            </div>

            <h3 class="text-centered">The Kernel Trick and Dot Products</h3>
            <p class="text-justified">
                While SVMs are inherently linear classifiers, they can effectively handle non-linear classification tasks through a mathematical technique called the kernel trick. The kernel trick allows SVMs to operate in an implicit higher-dimensional feature space without ever computing the coordinates of the data in that space.
            </p>
            
            <p class="text-justified">
                The key insight is that the SVM algorithm only depends on inner products (dot products) between data points, not on the data points themselves. The dot product measures the similarity between two vectors and is calculated as:
            </p>
            
            <div class="equation">
                x · y = Σ(x_i × y_i)
            </div>
            
            <p class="text-justified">
                A kernel function K(x, y) effectively computes the dot product in a higher-dimensional space without explicitly transforming the data:
            </p>
            
            <div class="equation">
                K(x, y) = φ(x) · φ(y)
            </div>
            
            <p class="text-justified">
                Where φ(x) represents the transformation to the higher-dimensional space. By substituting kernel functions for dot products, SVMs can learn non-linear decision boundaries in the original feature space.
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/svm_kernel_trick.png" alt="Kernel Trick Visualization">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This visualization demonstrates the kernel trick. The left side shows data that's not linearly separable in 2D space. The middle shows how these points can be mapped to a higher-dimensional space (3D in this example) where they become linearly separable. The right side shows the resulting nonlinear decision boundary when projected back to the original 2D space. 
                    </p>
                </div>
            </div>

            <h3 class="text-centered">Example: Polynomial Kernel Transformation</h3>
            <p class="text-justified">
                To illustrate how a kernel implicitly maps data to a higher-dimensional space, consider a 2D point (x₁, x₂) and a polynomial kernel with degree d = 2 and coefficient r = 1:
            </p>
            
            <div class="equation">
                K(x, y) = (x · y + r)^d = (x₁y₁ + x₂y₂ + 1)^2
            </div>
            
            <p class="text-justified">
                When expanded, this becomes:
            </p>
            
            <div class="equation">
                K(x, y) = (x₁y₁ + x₂y₂ + 1)^2 = (x₁y₁)^2 + (x₂y₂)^2 + 2(x₁y₁)(x₂y₂) + 2x₁y₁ + 2x₂y₂ + 1
            </div>
            
            <p class="text-justified">
                This expansion corresponds to the dot product in a 6-dimensional space where the original 2D point (x₁, x₂) is mapped to:
            </p>
            
            <div class="equation">
                φ(x) = (x₁², x₂², √2x₁x₂, √2x₁, √2x₂, 1)
            </div>
            
            <p class="text-justified">
                The beauty of the kernel trick is that this transformation is never explicitly computed. Instead, the kernel function directly calculates what the dot product would be in this higher-dimensional space. This makes SVMs computationally efficient even when the implicit feature space has very high or even infinite dimensions, as with the Radial Basis Function (RBF) kernel.
            </p>


            
            <h3 class="text-centered">Common Kernel Functions</h3>
            <p class="text-justified">
                Common kernels include linear, polynomial, radial basis function (RBF), and sigmoid. 
                Each kernel transforms the input space differently to enable the SVM to find an optimal separating boundary. 
                The linear kernel is best suited for linearly separable data, while the polynomial and RBF kernels allow the model to capture complex, non-linear relationships. 
                The choice of kernel has a significant impact on the model’s ability to generalize to unseen data.
            </p>
            <div class="two-column-container">
                <div class="method-card">
                    <h3 class="text-centered">Linear Kernel</h3>
                    <div class="equation">K(x, y) = x · y</div>
                    <p class="text-justified">
                        The simplest kernel, equivalent to no transformation. Effective when the data is already linearly separable. Computationally efficient and interpretable, as the original feature space is preserved.
                    </p>
                </div>
                <div class="method-card">
                    <h3 class="text-centered">Polynomial Kernel</h3>
                    <div class="equation">K(x, y) = (γx · y + r)^d</div>
                    <p class="text-justified">
                        Creates polynomial combinations of features up to degree d. The parameters γ (gamma), r (coefficient), and d (degree) control the flexibility of the decision boundary. Useful for problems where feature interactions matter.
                    </p>
                </div>
                <div class="method-card">
                    <h3 class="text-centered">RBF Kernel</h3>
                    <div class="equation">K(x, y) = exp(-γ||x - y||²)</div>
                    <p class="text-justified">
                        The Radial Basis Function (Gaussian) kernel measures similarity based on distance between points. Creates complex, localized decision boundaries. The parameter γ controls the influence radius of each support vector.
                    </p>
                </div>
                <div class="method-card">
                    <h3 class="text-centered">Sigmoid Kernel</h3>
                    <div class="equation">K(x, y) = tanh(γx · y + r)</div>
                    <p class="text-justified">
                        Inspired by neural networks, this kernel applies a hyperbolic tangent transformation. Its effectiveness depends heavily on proper hyperparameter tuning. Less commonly used than RBF or polynomial kernels.
                    </p>
                </div>
            </div>


            <h3 class="text-centered">SVM Classification of Audio Data</h3>
            <p class="text-justified">
                To apply SVM to audio classification, a dataset consisting of audio features from two categories is selected: "laughter" and "footsteps". This binary classification task provides a clear benchmark for evaluating different SVM configurations. The raw dataset is shown below.
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/svm_raw_data.png" alt="SVM Raw Dataset">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The dataset contains extracted audio features for both "laughter" and "footsteps" categories. Each row represents one audio sample, with columns corresponding to various audio features like spectral contrast, MFCCs, chroma, and other acoustic descriptors.
                    </p>
                </div>
            </div>

            <p class="text-justified">
                For the classification task, the categories are encoded as binary values: "laughter" as 0 and "footsteps" as 1. This encoded dataset serves as the foundation for the SVM models.
                The encoded dataset is shown below.
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/svm_encoded_data.png" alt="SVM Encoded Dataset">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The dataset after binary encoding of categories. The "category" column now contains numeric values (0 for "laughter", 1 for "footsteps") suitable for machine learning algorithms.
                    </p>
                </div>
            </div>


            <p class="text-justified">
                All supervised learning methods require splitting data into training and testing sets to evaluate model performance objectively. This ensures that models are evaluated on data they haven't seen during training, providing a reliable estimate of how well they'll perform on new, unseen data. 
                The dataset is split into training (70%) and testing (30%) sets using stratified sampling to maintain class distribution. The training and testing sets are shown below.
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/svm_train_data.png" alt="SVM Training Data">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The training dataset contains 70% of the original samples, maintaining the same class distribution. This set is used to train the SVM models.
                    </p>
                </div>
            
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/svm_test_data.png" alt="SVM Testing Data">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The testing dataset contains the remaining 30% of samples and is used to evaluate model performance on unseen data.
                    </p>
                </div>
            </div>

            <p class="text-justified">
                SVMs are sensitive to feature scaling, so proper preprocessing is essential. 
                The features are standardized using StandardScaler, which transforms each feature to have zero mean and unit variance. This preprocessing is crucial for SVM performance, as the algorithm's distance calculations are directly affected by feature magnitudes.
                All features are standardized to have zero mean and unit variance, which helps prevent features with larger scales from dominating the model. The scaled training and testing sets are shown below.
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/svm_train_data_scaled.png" alt="Scaled Training Data - SVM">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This images depicts the scaled training data. The features are scaled to a standard normal distribution with mean=0 and variance=1.
                    </p>
                </div>

                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/svm_test_data_scaled.png" alt="Scaled Testing Data - SVM">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This images depicts the scaled testing data. The features are scaled to a standard normal distribution using parameters calculated on the training data.
                    </p>
                </div>
            </div>


            <h3 class="text-centered">Implementing SVM with Different Kernels</h3>
            <p class="text-justified">
                Three different SVM kernels are implemented to explore their effectiveness on audio classification:
            </p>
            
            <ul class="ul-list">
                <li>Linear Kernel: The simplest form of SVM, creating straight-line decision boundaries in the original feature space.</li>
                <li>RBF Kernel (Radial Basis Function): A flexible, non-linear kernel that can capture complex relationships through implicitly mapping data to an infinite-dimensional space.</li>
                <li>Polynomial Kernel (degree=3): Creates feature combinations up to the specified degree, allowing for curved decision boundaries.</li>
            </ul>
            
            <p class="text-justified">
                For each kernel type, three different regularization parameter (C) values are tested: 0.5, 10, and 100. The C parameter controls the trade-off between achieving a smooth decision boundary and correctly classifying training points:
            </p>
            
            <ul class="ul-list">
                <li>C=0.5: A smaller value that prioritizes a smoother decision boundary, potentially at the cost of misclassifying more training points.</li>
                <li>C=10: A moderate value balancing between decision boundary smoothness and training accuracy.</li>
                <li>C=100: A larger value that emphasizes correctly classifying training points, potentially at the cost of overfitting.</li>
            </ul>
            
            <p class="text-justified">
                The implementation uses scikit-learn's SVC (Support Vector Classification) with probability estimates enabled for ROC curve analysis. Each model is evaluated on the test set using multiple metrics including accuracy, precision, recall, F1-score, confusion matrix, and area under the ROC curve (AUC).
            </p>
            
            <p class="text-justified">
                Additionally, for visualization purposes, the high-dimensional feature space is reduced to 2D using Principal Component Analysis (PCA). This allows plotting of the decision boundaries in a way that human eyes can interpret, though it's important to note that the actual classification happens in the original feature space.
            </p>
            
            <h3 class="text-centered">SVM with Linear Kernel and C=0.5</h3>
            <p class="text-justified">
                The linear kernel creates a straight-line decision boundary with minimal complexity. With C=0.5, this configuration prioritizes a smoother boundary over perfect classification of training points, establishing a baseline against which more complex kernel functions can be compared.
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/svm/svm_kernel_linear_c_0.5_classification_report_heatmap.png" alt="Classification Report - Linear Kernel C=0.5">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The classification report shows the performance metrics for the linear kernel SVM with C=0.5. For class 0 (laughter), the model achieved a precision of 0.928, recall of 0.920, and F1-score of 0.924. For class 1 (footsteps), the precision was 0.921, recall was 0.929, and F1-score was 0.925. The overall accuracy is 92.4%, with similar macro-average metrics, indicating balanced performance across both classes.
                    </p>
                </div>
                
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/svm/svm_kernel_linear_c_0.5_confusion_matrix.png" alt="Confusion Matrix - Linear Kernel C=0.5">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The confusion matrix provides a detailed breakdown of predictions. The model correctly classified 207 laughter samples and 209 footstep samples, while misclassifying 18 laughter samples as footsteps and 16 footstep samples as laughter.
                    </p>
                </div>
                
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/svm/svm_kernel_linear_c_0.5_roc_curve.png" alt="ROC Curve - Linear Kernel C=0.5">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The ROC curve illustrates the model's performance across different classification thresholds. The Area Under the Curve (AUC) is 0.972, indicating very strong classification performance. The curve's proximity to the top-left corner reflects the model's ability to differentiate between the two audio classes.
                    </p>
                </div>
                
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/svm/svm_kernel_linear_c_0.5_decision_boundary.png" alt="Decision Boundary - Linear Kernel C=0.5">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This visualization shows the decision boundary of the linear kernel SVM in a 2D space created by PCA. The linear kernel creates a straight-line boundary (in the reduced dimension space) separating the two classes. While the boundary appears clean, there are still some misclassifications in regions where the classes overlap. The regularization parameter C=0.5 results in a slightly smoother boundary that doesn't try to perfectly separate all training points.
                    </p>
                </div>
            </div>

            <h3 class="text-centered">Linear Kernel with C=10</h3>
            <p class="text-justified">
                With an increased C value of 10, this linear kernel model places greater emphasis on correctly classifying training points while still maintaining a straight-line decision boundary. This medium regularization strength allows the model to better fit the training data while still providing reasonable generalization to unseen examples.
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/svm/svm_kernel_linear_c_10_classification_report_heatmap.png" alt="Classification Report - Linear Kernel C=10">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        With C=10, the linear kernel SVM shows great performance. For class 0 (laughter), precision is 0.937, recall is 0.924, and F1-score is 0.931. For class 1 (footsteps), precision is 0.925, recall is 0.938, and F1-score is 0.932. The overall accuracy has increased to 93.1%, showing that the higher C value allows the model to better fit the training data while still generalizing well to the test set.
                    </p>
                </div>
                
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/svm/svm_kernel_linear_c_10_confusion_matrix.png" alt="Confusion Matrix - Linear Kernel C=10">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The confusion matrix shows 208 correctly classified laughter samples and 211 correctly classified footstep samples. Misclassifications have decreased to 17 laughter samples predicted as footsteps and 14 footstep samples predicted as laughter.
                    </p>
                </div>
                
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/svm/svm_kernel_linear_c_10_roc_curve.png" alt="ROC Curve - Linear Kernel C=10">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The ROC curve illustrates the model's performance across different classification thresholds. The Area Under the Curve (AUC) is 0.971, indicating strong classification performance. The curve's proximity to the top-left corner reflects the model's ability to differentiate between the two audio classes with great confidence.
                    </p>
                </div>

                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/svm/svm_kernel_linear_c_10_decision_boundary.png" alt="Decision Boundary - Linear Kernel C=10">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The decision boundary visualization shows the linear separator in the reduced 2D feature space created by PCA. The boundary creates a clear division between the two audio classes, with most points correctly classified on their respective sides. As characteristic of the linear kernel, the boundary forms a straight line through the feature space, attempting to maximize the margin while respecting the C=10 regularization parameter.
                    </p>
                </div>
            </div>

            <h3 class="text-centered">Linear Kernel with C=100</h3>
            <p class="text-justified">
                At C=100, the linear kernel model strongly prioritizes correct classification of training points over boundary smoothness. This high regularization parameter pushes the model to minimize training errors, potentially at the cost of generalization. Despite the high C value, the decision boundary remains linear in nature.
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/svm/svm_kernel_linear_c_100_classification_report_heatmap.png" alt="Classification Report - Linear Kernel C=100">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        With C=100, the linear kernel SVM shows great performance again. For class 0 (laughter), precision is 0.941, recall is 0.916, and F1-score is 0.928. For class 1 (footsteps), precision is 0.918, recall is 0.942, and F1-score is 0.930. The overall accuracy of the model is 92.9%, indicating strong classification performance for this binary audio task.
                    </p>
                </div>
                
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/svm/svm_kernel_linear_c_100_confusion_matrix.png" alt="Confusion Matrix - Linear Kernel C=100">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The confusion matrix shows 206 correctly classified laughter samples and 212 correctly classified footstep samples. Misclassifications include 19 laughter samples predicted as footsteps and 13 footstep samples predicted as laughter.
                    </p>
                </div>
                
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/svm/svm_kernel_linear_c_100_roc_curve.png" alt="ROC Curve - Linear Kernel C=100">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The ROC curve illustrates the model's performance across different classification thresholds. The Area Under the Curve (AUC) is 0.969, indicating strong classification performance. The curve's proximity to the top-left corner reflects the model's ability to differentiate between the two audio classes with great confidence.
                    </p>
                </div>
                
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/svm/svm_kernel_linear_c_100_decision_boundary.png" alt="Decision Boundary - Linear Kernel C=100">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The decision boundary visualization for the linear kernel with C=100 shows a straight-line separator dividing the feature space in the reduced PCA dimensions. The boundary creates a clear division between the two audio classes, with the majority of points properly classified on their respective sides. As expected from a linear kernel, the decision boundary remains a straight line regardless of the regularization strength, creating a simple but effective separation for this binary audio classification task.
                    </p>
                </div>
            </div>

            <h3 class="text-centered">RBF Kernel with C=0.5</h3>
            <p class="text-justified">
                The Radial Basis Function (RBF) kernel enables non-linear decision boundaries by implicitly mapping data to an infinite-dimensional space. With C=0.5, the model creates smooth, curved boundaries that can better adapt to the underlying structure of audio data while avoiding overfitting to noise or outliers.
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/svm/svm_kernel_rbf_c_0.5_classification_report_heatmap.png" alt="Classification Report - RBF Kernel C=0.5">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The RBF kernel with C=0.5 shows impressive performance as well. For class 0 (laughter), precision is 0.911, recall is 0.951, and F1-score is 0.930. For class 1 (footsteps), precision is 0.949, recall is 0.907, and F1-score is 0.927. The overall accuracy is 92.9%, demonstrating the RBF kernel's ability to capture relationships in the audio feature space.
                    </p>
                </div>
                
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/svm/svm_kernel_rbf_c_0.5_confusion_matrix.png" alt="Confusion Matrix - RBF Kernel C=0.5">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The confusion matrix shows 214 correctly classified laughter samples and 204 correctly classified footstep samples. Misclassifications include 11 laughter samples predicted as footsteps and 21 footstep samples predicted as laughter.
                    </p>
                </div>
                
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/svm/svm_kernel_rbf_c_0.5_roc_curve.png" alt="ROC Curve - RBF Kernel C=0.5">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The ROC curve shows exceptional discriminative power with an AUC of 0.986. The curve hugs the top-left corner indicating the RBF kernel's ability to distinguish between the two audio classes even with a relatively conservative regularization parameter.
                    </p>
                </div>
                
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/svm/svm_kernel_rbf_c_0.5_decision_boundary.png" alt="Decision Boundary - RBF Kernel C=0.5">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The decision boundary visualization reveals a fundamentally different shape compared to the linear models. The RBF kernel creates curved, non-linear boundaries that can enclose regions and better adapt to the data distribution. With C=0.5, the boundary remains relatively smooth while still capturing the underlying non-linear structure of the audio feature space.
                    </p>
                </div>
            </div>

            <h3 class="text-centered">RBF Kernel with C=10</h3>
            <p class="text-justified">
                The RBF kernel with C=10 represents a balanced configuration that permits more complex decision boundaries while still maintaining good generalization. This setting allows the model to create more intricate decision regions that better capture the non-linear relationships present in audio feature spaces.
            </p>
            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/svm/svm_kernel_rbf_c_10_classification_report_heatmap.png" alt="Classification Report - RBF Kernel C=10">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        With C=10, the RBF kernel SVM improves its performance significantly. For class 0 (laughter), precision is 0.934, recall is 0.951, and F1-score is 0.943. For class 1 (footsteps), precision is 0.950, recall is 0.933, and F1-score is 0.942. The overall accuracy reaches an impressive 94.2%, showing that the increased regularization parameter allows the model to better adapt to the complex patterns in audio features.
                    </p>
                </div>
                
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/svm/svm_kernel_rbf_c_10_confusion_matrix.png" alt="Confusion Matrix - RBF Kernel C=10">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The confusion matrix shows improvement with 214 correctly classified laughter samples and 210 correctly classified footstep samples. Misclassifications have decreased to 11 laughter samples predicted as footsteps and 15 footstep samples predicted as laughter.
                    </p>
                </div>
                
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/svm/svm_kernel_rbf_c_10_roc_curve.png" alt="ROC Curve - RBF Kernel C=10">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The ROC curve shows exceptional performance with an AUC of 0.986. The curve hugs the top-left corner, indicating the RBF kernel with C=10 achieves excellent separation between the two audio classes across various probability thresholds.
                    </p>
                </div>
                
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/svm/svm_kernel_rbf_c_10_decision_boundary.png" alt="Decision Boundary - RBF Kernel C=10">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The decision boundary visualization for the RBF kernel with C=10 reveals the model's ability to create curved, non-linear separations in the feature space. The boundary forms complex regions that adapt to the data distribution, creating localized areas of classification rather than a single straight line. This flexibility allows the RBF kernel to capture the nuanced acoustic patterns that differentiate between laughter and footstep sounds in the audio feature space.
                    </p>
                </div>
            </div>

            <h3 class="text-centered">RBF Kernel with C=100</h3>
            <p class="text-justified">
                At C=100, the RBF kernel model generates highly flexible decision boundaries that can closely fit the training data. This configuration allows the model to create complex, localized regions that precisely separate audio classes, capturing subtle patterns in the feature space that simpler models might miss.
            </p>
            
            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/svm/svm_kernel_rbf_c_100_classification_report_heatmap.png" alt="Classification Report - RBF Kernel C=100">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        With C=100, the RBF kernel SVM performs similarly. For class 0 (laughter), precision is 0.927, recall is 0.960, and F1-score is 0.943. For class 1 (footsteps), precision is 0.959, recall is 0.924, and F1-score is 0.941. The overall accuracy stays 94.2%, demonstrating that further increasing the regularization parameter provides minimal benefits for this audio classification task.
                    </p>
                </div>
                
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/svm/svm_kernel_rbf_c_100_confusion_matrix.png" alt="Confusion Matrix - RBF Kernel C=100">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The confusion matrix shows similar performance with 216 correctly classified laughter samples and 208 correctly classified footstep samples. Misclassifications include 9 laughter samples predicted as footsteps and 17 footstep samples predicted as laughter.
                    </p>
                </div>
                
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/svm/svm_kernel_rbf_c_100_roc_curve.png" alt="ROC Curve - RBF Kernel C=100">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The ROC curve shows the same outstanding performance with an AUC of 0.986. The curve hugs the top-left corner, indicating the RBF kernel with C=10 achieves excellent separation between the two audio classes across various probability thresholds.
                    </p>
                </div>
                
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/svm/svm_kernel_rbf_c_100_decision_boundary.png" alt="Decision Boundary - RBF Kernel C=100">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The decision boundary visualization for the RBF kernel with C=100 reveals the model's ability to create curved, non-linear separations in the feature space. The boundary forms complex regions that adapt to the data distribution, creating localized areas of classification rather than a single straight line. This flexibility allows the RBF kernel to capture the nuanced acoustic patterns that differentiate between laughter and footstep sounds in the audio feature space.
                    </p>
                </div>
            </div>

            <h3 class="text-centered">Polynomial Kernel with C=0.5</h3>
            <p class="text-justified">
                The polynomial kernel (degree=3) creates curved decision boundaries by modeling feature interactions up to the specified degree. With C=0.5, the model maintains relatively smooth boundaries while capturing non-linear relationships, offering a middle ground between the rigidity of linear kernels and the flexibility of RBF kernels.
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/svm/svm_kernel_poly_degree_3_c_0.5_classification_report_heatmap.png" alt="Classification Report - Polynomial Kernel C=0.5">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The polynomial kernel (degree=3) with C=0.5 demonstrates decent performance. For class 0 (laughter), precision is 0.844, recall is 0.964, and F1-score is 0.900. For class 1 (footsteps), precision is 0.959, recall is 0.822, and F1-score is 0.885. The overall accuracy is 89.3%, demonstrating polynomial kernel's ability to capture patterns in the audio data.
                    </p>
                </div>
                
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/svm/svm_kernel_poly_degree_3_c_0.5_confusion_matrix.png" alt="Confusion Matrix - Polynomial Kernel C=0.5">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The confusion matrix shows 217 correctly classified laughter samples and 185 correctly classified footstep samples. Misclassifications include 8 laughter samples predicted as footsteps and 40 footstep samples predicted as laughter.
                    </p>
                </div>
                
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/svm/svm_kernel_poly_degree_3_c_0.5_roc_curve.png" alt="ROC Curve - Polynomial Kernel C=0.5">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The ROC curve shows strong discriminative ability with an AUC of 0.973. It demonstrates the polynomial kernel's ability to separate the audio classes, particularly with this moderate regularization parameter.
                    </p>
                </div>
                
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/svm/svm_kernel_poly_degree_3_c_0.5_decision_boundary.png" alt="Decision Boundary - Polynomial Kernel C=0.5">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The decision boundary visualization shows curved, polynomial-shaped boundaries that are more flexible than linear boundaries but less localized than RBF boundaries. With C=0.5, the boundary maintains a smooth, flowing shape that captures the general trend in the data while avoiding overly complex fits to individual points.
                    </p>
                </div>
            </div>

            <h3 class="text-centered">Polynomial Kernel with C=10</h3>
            <p class="text-justified">
                With C=10, the polynomial kernel model (degree=3) strikes a balance between boundary complexity and generalization ability. This configuration allows for more pronounced curves and loops in the decision boundary, better accounting for the polynomial relationships between audio features.
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/svm/svm_kernel_poly_degree_3_c_10_classification_report_heatmap.png" alt="Classification Report - Polynomial Kernel C=10">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        With C=10, the polynomial kernel SVM shows improved performance. For class 0 (laughter), precision is 0.884, recall is 0.947, and F1-score is 0.914. For class 1 (footsteps), precision is 0.943, recall is 0.876, and F1-score is 0.908. The overall accuracy increases to 91.1%, demonstrating that a higher regularization parameter allows the polynomial kernel to better capture the complex patterns in audio features.
                    </p>
                </div>
                
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/svm/svm_kernel_poly_degree_3_c_10_confusion_matrix.png" alt="Confusion Matrix - Polynomial Kernel C=10">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The confusion matrix shows 213 correctly classified laughter samples and 197 correctly classified footstep samples. Misclassifications have decreased to 28 laughter samples predicted as footsteps and 12 footstep samples predicted as laughter.
                    </p>
                </div>
                
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/svm/svm_kernel_poly_degree_3_c_10_roc_curve.png" alt="ROC Curve - Polynomial Kernel C=10">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The ROC curve shows solid performance with an AUC of 0.975. The curve approaches the top-left corne, indicating that the polynomial kernel with C=10 achieves excellent separation between the two audio classes across various probability thresholds.
                    </p>
                </div>
                
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/svm/svm_kernel_poly_degree_3_c_10_decision_boundary.png" alt="Decision Boundary - Polynomial Kernel C=10">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The decision boundary visualization for the polynomial kernel (degree=3) with C=10 displays characteristic curved shapes with smooth transitions throughout the feature space. The boundary forms polynomial curves that separate the audio classes with moderate complexity, creating rounded regions that follow the underlying data distribution. This mathematical structure enables the model to capture non-linear relationships and feature interactions present in the audio data.
                    </p>
                </div>
            </div>

            <h3 class="text-centered">Polynomial Kernel with C=100</h3>
            <p class="text-justified">
                The polynomial kernel with C=100 and degree=3 produces intricate decision boundaries with strong emphasis on training accuracy. This configuration creates elaborately curved boundaries that attempt to correctly classify as many training points as possible while maintaining the characteristic polynomial shapes determined by the kernel function.
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/svm/svm_kernel_poly_degree_3_c_100_classification_report_heatmap.png" alt="Classification Report - Polynomial Kernel C=100">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        With C=100, the polynomial kernel SVM achieves good performance. For class 0 (laughter), precision is 0.882, recall is 0.929, and F1-score is 0.905. For class 1 (footsteps), precision is 0.925, recall is 0.876, and F1-score is 0.900. The overall accuracy reaches 90.2%, indicating that increasing the regularization parameter led to diminishing returns.
                    </p>
                </div>
                
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/svm/svm_kernel_poly_degree_3_c_100_confusion_matrix.png" alt="Confusion Matrix - Polynomial Kernel C=100">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The confusion matrix shows good performance with 209 correctly classified laughter samples and 197 correctly classified footstep samples. Misclassifications have decreased to 16 laughter samples predicted as footsteps and 28 footstep samples predicted as laughter.
                    </p>
                </div>
                
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/svm/svm_kernel_poly_degree_3_c_100_roc_curve.png" alt="ROC Curve - Polynomial Kernel C=100">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The ROC curve shows great discriminative ability with an AUC of 0.970. The curve approaches the top-left corne, indicating that the polynomial kernel with C=100 achieves excellent separation between the two audio classes across various probability thresholds.
                    </p>
                </div>
                
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/svm/svm_kernel_poly_degree_3_c_100_decision_boundary.png" alt="Decision Boundary - Polynomial Kernel C=100">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The decision boundary visualization for the polynomial kernel (degree=3) with C=100 displays characteristic curved shapes with smooth transitions throughout the feature space. The boundary forms polynomial curves that separate the audio classes with moderate complexity, creating rounded regions that follow the underlying data distribution. This mathematical structure enables the model to capture non-linear relationships and feature interactions present in the audio data.
                    </p>
                </div>
            </div>

            <h3 class="text-centered">Comparison of SVM Models</h3>
            <p class="text-justified">
                After evaluating nine different SVM configurations with varying kernels and regularization parameters, a clear pattern emerges regarding their effectiveness for audio classification. The following table summarizes the results across all models:
            </p>

            <div class="plot-container">
                <div class="comparison-table">
                    <table>
                        <tr>
                            <th>Kernel</th>
                            <th>C Value</th>
                            <th>Accuracy (%)</th>
                            <th>F1-Score (Class 0)</th>
                            <th>F1-Score (Class 1)</th>
                        </tr>
                        <tr>
                            <td>Linear</td>
                            <td>0.5</td>
                            <td>92.4</td>
                            <td>0.924</td>
                            <td>0.925</td>
                        </tr>
                        <tr>
                            <td>Linear</td>
                            <td>10</td>
                            <td>93.1</td>
                            <td>0.931</td>
                            <td>0.932</td>
                        </tr>
                        <tr>
                            <td>Linear</td>
                            <td>100</td>
                            <td>92.9</td>
                            <td>0.928</td>
                            <td>0.930</td>
                        </tr>
                        <tr>
                            <td>Polynomial (d=3)</td>
                            <td>0.5</td>
                            <td>89.3</td>
                            <td>0.900</td>
                            <td>0.885</td>
                        </tr>
                        <tr>
                            <td>Polynomial (d=3)</td>
                            <td>10</td>
                            <td>91.1</td>
                            <td>0.914</td>
                            <td>0.908</td>
                        </tr>
                        <tr>
                            <td>Polynomial (d=3)</td>
                            <td>100</td>
                            <td>90.2</td>
                            <td>0.905</td>
                            <td>0.900</td>
                        </tr>
                        <tr>
                            <td>RBF</td>
                            <td>0.5</td>
                            <td>92.9</td>
                            <td>0.930</td>
                            <td>0.927</td>
                        </tr>
                        <tr>
                            <td>RBF</td>
                            <td>10</td>
                            <td>94.2</td>
                            <td>0.943</td>
                            <td>0.942</td>
                        </tr>
                        <tr>
                            <td>RBF</td>
                            <td>100</td>
                            <td>94.2</td>
                            <td>0.943</td>
                            <td>0.941</td>
                        </tr>
                    </table>
                </div>
            </div>
            
            <p class="text-justified">
                Key insights from this comparison include:
            </p>
            
            <ul class="ul-list">
                <li>Kernel Performance: The RBF kernel performs best, especially at higher C values, followed closely by the linear kernel. The polynomial kernel shows lower performance across all regularization parameters. This suggests that while the audio feature space contains non-linear relationships that benefit from the RBF kernel, the feature space may be relatively well-structured as linear kernels also perform effectively.</li>
                <li>Regularization Impact: For RBF and linear kernels, increasing the regularization parameter C from 0.5 to 10 provides notable performance improvements. Further increasing to C=100 maintains similar performance or shows slight decreases, indicating an optimal point around C=10.</li>
                <li>Model Complexity: Unlike the typical performance ranking often seen in other domains, in this audio classification task, the performance ranking is RBF > Linear > Polynomial. This unexpected pattern suggests that the polynomial kernel may be creating overly complex decision boundaries that don't generalize as well for this specific audio data.</li>
                <li>Class Balance: The F1-scores for both classes are remarkably similar across most models, indicating well-balanced class performance. This suggests the models are equally effective at classifying both audio categories.</li>
                <li>Optimal Configuration: The RBF kernel with C=10 emerges as the optimal choice, achieving the highest accuracy (94.2%) with more efficient computation compared to C=100, which offers no performance improvement.</li>
            </ul>
            
            <h3 class="text-centered">Conclusion</h3>
            <p class="text-justified">
                Support Vector Machines proved highly effective for audio classification tasks, with the best configuration (RBF kernel, C=10 or C=100) achieving 94.2% accuracy. This analysis demonstrates several key insights:
            </p>
            
            <ul class="ul-list">
                <li>RBF kernels perform best, but linear kernels deliver surprisingly strong results (93.1%), suggesting the audio features have good linear separability.</li>
                <li>The polynomial kernel consistently underperforms both alternatives for this specific audio data.</li>
                <li>Optimal performance occurs at moderate regularization (C=10), with minimal improvement at higher values.</li>
                <li>Balanced F1-scores across classes indicate effective classification without bias toward either category.</li>
            </ul>
            
            <p class="text-justified">
                These findings suggest that RBF kernels with moderate C values should be the starting point for audio classification, with linear kernels as an efficient alternative offering strong performance with lower computational requirements. The results highlight the importance of appropriate kernel selection and hyperparameter tuning in SVM applications.
            </p>

            <p class="text-justified">
                The full script to perform SVM classification, including preprocessing and visualizations, can be found 
                <a href="https://github.com/AnirudhKakati/sound-wave-analysis/blob/main/scripts/svm.py" target="_blank">here.</a>
            </p>
        </div>
        <!-- End of SVM Section -->

        <!-- Ensemble (Random Forest) Section -->
        <div id="ensemble" class="content-section" data-aos="fade-up">
            <h2>Ensembles</h2>
            
            <!-- <h3 class="text-centered">Overview</h3>
            <p class="text-justified">
                Support Vector Machines (SVMs) are powerful supervised learning algorithms used for classification, regression, and outlier detection. SVMs are particularly effective in high-dimensional spaces and cases where the number of dimensions exceeds the number of samples. The core idea behind SVMs is to find the optimal hyperplane that maximizes the margin between different classes in the feature space. This margin maximization gives SVMs strong generalization properties, making them less prone to overfitting compared to many other classifiers.
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/svm_concept.png" alt="SVM Concept">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This illustration shows the fundamental concept of SVMs. The algorithm finds the hyperplane (solid line) that maximizes the margin between classes. The margin is the distance between the hyperplane and the closest data points from each class, known as support vectors (circled). These support vectors are the only data points that determine the hyperplane's position, making SVMs robust to outliers far from the decision boundary.
                    </p>
                </div>
            </div>

            <p class="text-justified">
                One of SVM's most powerful features is the kernel trick, which allows the algorithm to operate in an implicit higher-dimensional feature space without ever computing the coordinates of the data in that space. This enables SVMs to find nonlinear decision boundaries in the original feature space by mapping the data into a higher-dimensional space where a linear boundary can separate the classes.
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/kernel_trick.png" alt="Kernel Trick Visualization">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This visualization demonstrates the kernel trick. The left side shows data that's not linearly separable in 2D space. The middle shows how these points can be mapped to a higher-dimensional space (3D in this example) where they become linearly separable. The right side shows the resulting nonlinear decision boundary when projected back to the original 2D space. Common kernels include linear, polynomial, radial basis function (RBF), and sigmoid.
                    </p>
                </div>
            </div>
            
            <h3 class="text-centered">Data Preparation</h3>
            <p class="text-justified">
                SVMs are sensitive to feature scaling, so proper preprocessing is essential. For our audio classification task, we standardized all features to have zero mean and unit variance, which helps prevent features with larger scales from dominating the model. We maintained the same train-test split used for previous models to ensure fair comparison.
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/svm_data_prep.png" alt="SVM Data Preparation">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This image shows the effect of standardization on the feature distributions. The left side shows the original feature distributions with varying scales and centers. The right side shows the standardized features, all centered at zero with similar scales. This preprocessing is crucial for SVM performance, as the algorithm's distance calculations are directly affected by feature magnitudes.
                    </p>
                </div>
            </div>

            <h3 class="text-centered">SVM Implementation</h3>
            <p class="text-justified">
                We implemented SVM classification for our audio dataset using scikit-learn's SVC (Support Vector Classification) with different kernel functions. Due to the computational intensity of training SVMs on large datasets, we used a stratified subset of the data with 500 samples per class.
            </p>
            


            <p class="text-justified">
                The complete implementation with additional metrics, visualizations, and hyperparameter tuning can be found in the 
                <a href="https://github.com/AnirudhKakati/sound-wave-analysis/blob/main/scripts/svm.py" target="_blank">GitHub repository</a>.
            </p>

            <h3 class="text-centered">Results</h3>
            <p class="text-justified">
                We evaluated the performance of SVMs with different kernels on our audio classification task:
            </p>

            <div class="visualization-section">
                <h4 class="text-centered">Accuracy Comparison</h4>
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/svm_accuracy.png" alt="SVM Accuracy Comparison">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This chart compares the accuracy of SVM models with different kernels. The RBF kernel achieved the highest accuracy at 84.7%, followed by the polynomial kernel at 82.1%, and the linear kernel at 79.5%. The optimized RBF model after hyperparameter tuning reached 86.3%, demonstrating the importance of parameter selection in SVM performance.
                    </p>
                </div>
            </div>

            <div class="visualization-section">
                <h4 class="text-centered">Confusion Matrix - RBF Kernel</h4>
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/svm_confusion_matrix.png" alt="SVM Confusion Matrix">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The confusion matrix for the RBF kernel SVM shows strong classification performance across most sound categories. The model excels at recognizing distinct sounds like "sirens" and "piano" with few confusions. Similar to other classifiers, there are some confusions between acoustically similar categories like "traffic_sounds" and "train_sounds". The RBF kernel's ability to capture complex, nonlinear relationships in the feature space contributes to its superior performance.
                    </p>
                </div>
            </div>

            <div class="visualization-section">
                <h4 class="text-centered">Decision Boundaries Visualization</h4>
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/svm_decision_boundaries.png" alt="SVM Decision Boundaries">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This visualization shows decision boundaries for different SVM kernels in a simplified 2D space using the first two principal components. The linear kernel (left) creates straight-line boundaries, while the polynomial (middle) and RBF (right) kernels create curved boundaries that can capture more complex relationships. The RBF kernel produces the most flexible decision boundary, allowing it to better separate the sound categories in this reduced feature space.
                    </p>
                </div>
            </div>

            <div class="visualization-section">
                <h4 class="text-centered">Hyperparameter Sensitivity</h4>
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/svm_hyperparameters.png" alt="SVM Hyperparameter Sensitivity">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This heatmap shows the impact of different C and gamma values on the RBF kernel SVM's accuracy. The regularization parameter C controls the trade-off between margin maximization and training error minimization, while gamma determines the influence radius of each support vector. Higher accuracy is observed with moderate C values (10-100) and lower gamma values (0.01-0.1), indicating that a moderately complex model with broader influence regions performs best for audio classification.
                    </p>
                </div>
            </div>

            <h3 class="text-centered">Conclusion</h3>
            <p class="text-justified">
                The SVM analysis yielded several important insights for audio classification:
            </p>
            
            <ul class="ul-list">
                <li>SVMs with the RBF kernel achieved the highest accuracy among all individual models tested (86.3% after optimization), demonstrating their effectiveness for audio classification tasks.</li>
                <li>The superior performance of nonlinear kernels (RBF and polynomial) compared to the linear kernel indicates that audio features have complex, nonlinear relationships that benefit from more flexible decision boundaries.</li>
                <li>Hyperparameter tuning significantly improved SVM performance, highlighting the importance of parameter selection, particularly for the regularization parameter C and the kernel parameter gamma.</li>
                <li>SVMs showed strong generalization ability, maintaining good performance across diverse sound categories despite being trained on a smaller subset of the data.</li>
                <li>The primary drawback of SVMs was their computational intensity, requiring significantly longer training times than Naive Bayes or Decision Trees, particularly for large datasets with many classes.</li>
            </ul>
            
            <p class="text-justified">
                SVMs provide a powerful approach for audio classification, particularly when accuracy is prioritized over training efficiency. Their ability to handle high-dimensional data with complex relationships makes them well-suited for capturing the nuanced spectral and temporal patterns in audio signals. While SVMs don't provide the same level of interpretability as Decision Trees, their strong performance and theoretical foundations in statistical learning theory make them a valuable tool in the audio classification toolkit.
            </p> -->
        </div>

        <!-- <div class="content-section" data-aos="fade-up">
            <h3 class="text-centered">Overall Comparison of Supervised Learning Methods</h3>
            <p class="text-justified">
                After evaluating four different supervised learning approaches on the audio classification task, we can compare their performance, strengths, and limitations:
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/overall_comparison.png" alt="Overall Comparison">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This chart compares the best-performing model from each approach. The optimized SVM with RBF kernel achieved the highest accuracy at 86.3%, followed by Decision Trees at 82.3%, Logistic Regression at 78.9% (on the full multi-class problem), and Gaussian Naive Bayes at 76.8%. This progression reflects the increasing model complexity and flexibility in capturing nonlinear relationships in the data.
                    </p>
                </div>
            </div>

            <div class="comparison-table">
                <table>
                    <tr>
                        <th>Method</th>
                        <th>Strengths</th>
                        <th>Limitations</th>
                        <th>Best Use Cases</th>
                    </tr>
                    <tr>
                        <td>Naive Bayes</td>
                        <td>Fast training, works well with limited data, probabilistic output</td>
                        <td>Independence assumption often violated, lower accuracy</td>
                        <td>Baseline model, real-time classification, text-based audio metadata</td>
                    </tr>
                    <tr>
                        <td>Decision Trees</td>
                        <td>Highly interpretable, handles mixed feature types, no scaling required</td>
                        <td>Prone to overfitting, unstable (small changes in data can cause large changes in tree)</td>
                        <td>Exploratory analysis, rule extraction, feature importance assessment</td>
                    </tr>
                    <tr>
                        <td>Logistic Regression</td>
                        <td>Well-calibrated probabilities, efficient for multi-class problems, interpretable weights</td>
                        <td>Limited to linear decision boundaries, sensitive to feature scaling</td>
                        <td>Binary audio classification, probability estimation, baseline model</td>
                    </tr>
                    <tr>
                        <td>SVM</td>
                        <td>Highest accuracy, effective in high dimensions, robust generalization</td>
                        <td>Computationally intensive, sensitive to hyperparameters, less interpretable</td>
                        <td>Optimal classification accuracy, complex audio patterns, black-box classification</td>
                    </tr>
                </table>
            </div>

            <p class="text-justified">
                Each method offers a unique balance of interpretability, performance, and computational efficiency. For audio classification, the most appropriate model depends on specific requirements:
            </p>
            <ul class="ul-list">
                <li>If computational efficiency and real-time classification are priorities, Naive Bayes provides a good baseline.</li>
                <li>If understanding feature relationships and extracting rules are important, Decision Trees offer the most transparency.</li>
                <li>If well-calibrated probability estimates are needed, Logistic Regression is ideal.</li>
                <li>If maximum classification accuracy is the goal and computational resources are available, SVMs with the RBF kernel deliver the best performance.</li>
            </ul>

            <p class="text-justified">
                In practice, ensemble methods that combine these approaches or deeper neural network architectures might achieve even better performance for complex audio classification tasks. However, the methods explored here provide strong foundations and valuable insights into the acoustic properties that differentiate various sound categories.
            </p>
        </div> -->



        <div class="navigation-buttons">
            <button class="nav-button" onclick="location.href='supervised.html'">
                ← Back to Supervised Learning
            </button>
            <button class="nav-button" onclick="location.href='conclusion.html'">
                Next: Conclusion →
            </button>
        </div>
        
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/aos/2.3.4/aos.js"></script>
    <script>
        // Initialize AOS
        AOS.init();
        
        // Smooth scrolling for anchor links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function(e) {
                e.preventDefault();
                
                const targetId = this.getAttribute('href');
                const targetElement = document.querySelector(targetId);
                
                if (targetElement) {
                    window.scrollTo({
                        top: targetElement.offsetTop - 100, // Offset for fixed navbar
                        behavior: 'smooth'
                    });
                }
            });
        });
    </script>
</body>
</html>