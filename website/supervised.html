<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Supervised Learning - ML Project</title>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/aos/2.3.4/aos.css" rel="stylesheet">
    <link href="css/styles.css" rel="stylesheet">
    <style>
        /* Dropdown styles */
        .dropdown {
            position: relative;
            display: inline-block;
        }
        
        .dropdown-content {
            display: none;
            position: absolute;
            top: 100%;
            margin-top: 10px;
            background: rgba(0, 0, 0, 0.85);
            min-width: 160px;
            box-shadow: 0 8px 16px rgba(0, 0, 0, 0.3);
            z-index: 1;
            border-radius: 0.5rem;
            backdrop-filter: blur(10px);
        }
        
        .dropdown-content a {
            color: white;
            padding: 12px 16px;
            text-decoration: none;
            display: block;
            transition: background 0.3s ease;
        }
        
        .dropdown-content a:hover {
            background: var(--primary);
        }
        
        /* Add padding to create hover area */
        .dropdown-content::before {
            content: '';
            position: absolute;
            top: -10px;
            left: 0;
            width: 100%;
            height: 10px;
        }
        
        .dropdown:hover .dropdown-content {
            display: block;
        }
        
        .content-section {
            background: rgba(255, 255, 255, 0.05);
            padding: 3rem;
            border-radius: 1rem;
            margin: 2rem 0;
            backdrop-filter: blur(10px);
            border: 1px solid rgba(255, 255, 255, 0.1);
        }

        .visualization-section {
            margin: 4rem 0;
            padding: 2rem;
            background: rgba(255, 255, 255, 0.02);
            border-radius: 1rem;
            border: 1px solid rgba(255, 255, 255, 0.05);
        }
        
        .visualization-section img{
            transition: transform 0.3s ease;
        }

        .visualization-section img:hover{
            transform: scale(1.025);
        }

        .select-container {
            margin: 2rem 0;
        }

        select {
            background: var(--accent);
            color: white;
            padding: 0.8rem 2rem;
            border: none;
            border-radius: 0.5rem;
            font-size: 1.2rem;
            cursor: pointer;
            transition: transform 0.3s ease;
        }

        select:hover {
            transform: scale(1.05);
        }

        .plot-container {
            width: 100%; 
            text-align: center;
            max-width: 1000px;
            margin: 2rem auto;
            border-radius: 1rem;
            overflow: hidden;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.3);
            transition: transform 0.3s ease;
            background: rgba(0, 0, 0, 0.2);
            padding: 1rem;
        }

        .ul-list {
            font-size: 1.5rem;
            max-width: 800px;
            margin: 1rem auto;
            text-align: justify;
        }
        .plot-container img {
            width: 90%;
            height: auto;
            display: block;
            margin: 0 auto;
            border-radius: 0.5rem;
        }

        .plot-description {
            margin: 2rem 0;
            padding: 2rem;
            background: rgba(255, 255, 255, 0.03);
            border-radius: 1rem;
            border: 1px solid rgba(255, 255, 255, 0.05);
        }
        
        h2 {
            color: var(--secondary);
            margin: 3rem 0 1.5rem 0;
            font-size: 2.5rem;
        }

        h3 {
            color: var(--primary);
            margin: 2rem 0 1rem 0;
            font-size: 2rem;
        }

        .navigation-buttons {
            display: flex;
            justify-content: space-between;
            margin: 3rem 0;
        }

        .nav-button {
            padding: 1rem 2rem;
            background: var(--accent);
            border: none;
            border-radius: 1rem;
            color: white;
            cursor: pointer;
            font-size: 1.2rem;
            transition: transform 0.3s ease;
        }

        .nav-button:hover {
            transform: scale(1.1);
        }

        .text-centered {
            text-align: center;
        }

        .text-justified {
            text-align: justify;
        }

        a {
            color: var(--primary);
        }

        .result-table {
            width: 100%;
            margin: 2rem 0;
            border-collapse: collapse;
            background: rgba(255, 255, 255, 0.03);
            border-radius: 0.5rem;
            overflow: hidden;
        }

        .result-table th, .result-table td {
            padding: 1rem;
            text-align: left;
            border-bottom: 1px solid rgba(255, 255, 255, 0.1);
        }

        .result-table th {
            background: rgba(255, 255, 255, 0.05);
            font-weight: bold;
        }
        
        .two-column-container {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 2rem;
            margin: 2rem 0;
        }
        
        .method-card {
            background: rgba(255, 255, 255, 0.03);
            padding: 2rem;
            border-radius: 1rem;
            border: 1px solid rgba(255, 255, 255, 0.05);
            transition: transform 0.3s ease;
        }
        
        .method-card:hover {
            transform: scale(1.02);
        }

        .code-block {
            background: rgba(0, 0, 0, 0.3);
            padding: 1.5rem;
            border-radius: 0.5rem;
            overflow-x: auto;
            font-family: monospace;
            margin: 1.5rem 0;
            border-left: 3px solid var(--primary);
        }

        .comparison-table {
            width: 100%;
            margin: 2rem 0;
            border-collapse: collapse;
            border-radius: 0.5rem;
            overflow: hidden;
        }

        .comparison-table th, .comparison-table td {
            padding: 1rem;
            text-align: left;
            border: 1px solid rgba(255, 255, 255, 0.1);
        }

        .comparison-table th {
            background: rgba(255, 255, 255, 0.1);
            color: var(--secondary);
        }

        .comparison-table tr:nth-child(even) {
            background: rgba(255, 255, 255, 0.03);
        }

        .equation {
            background: rgba(255, 255, 255, 0.05);
            padding: 1rem;
            border-radius: 0.5rem;
            margin: 1rem 0;
            text-align: center;
            font-family: monospace;
            font-size: 1.2rem;
        }

    </style>
</head>
<body>
    <nav>
        <ul>
            <li><a href="index.html">Home</a></li>
            <li><a href="introduction.html">Introduction</a></li>
            <li><a href="data-exploration.html">Data Preparation & Exploration</a></li>
            <li class="dropdown">
                <a href="unsupervised.html">Unsupervised Learning</a>
                <div class="dropdown-content">
                    <a href="unsupervised.html#pca">PCA</a>
                    <a href="unsupervised.html#clustering">Clustering</a>
                    <a href="unsupervised.html#arm">ARM</a>
                </div>
            </li>
            <li class="dropdown">
                <a href="supervised.html" class="active">Supervised Learning</a>
                <div class="dropdown-content">
                    <a href="supervised.html#naive-bayes">Naive Bayes</a>
                    <a href="supervised.html#decision-trees">Decision Trees</a>
                    <a href="supervised.html#regression">Regression</a>
                    <!-- <a href="supervised.html#svm">SVM</a> -->
                </div>
            </li>
            <li><a href="conclusion.html">Conclusion</a></li>
            <li><a href="about.html">About Me</a></li>
        </ul>
    </nav>

    <div class="content-container">
        <div class="content-section" data-aos="fade-up">
            <br><br><br>
            <h1>Supervised Learning</h1>
            <p class="text-justified">
                Supervised learning is a cornerstone of machine learning where models learn from labeled training data to make predictions on unseen data. This section explores four powerful supervised techniques applied to the audio dataset: Naive Bayes for probabilistic classification, Decision Trees for rule-based decisions, Regression for predicting continuous values, and Support Vector Machines (SVM) for finding optimal decision boundaries. Each method offers unique strengths for audio classification tasks.
            </p>
        </div>

        <!-- Naive Bayes Section -->
        <div id="naive-bayes" class="content-section" data-aos="fade-up">
            <h2>Naive Bayes</h2>
            
            <h3 class="text-centered">Overview</h3>
            <!-- <p class="text-justified">
                Naive Bayes is a family of probabilistic classifiers based on applying Bayes' theorem with a "naive" assumption that features are conditionally independent given the class. Despite this simplifying assumption, Naive Bayes classifiers often perform surprisingly well in practice, especially for text classification, spam filtering, sentiment analysis, and recommendation systems. They require relatively small amounts of training data to estimate the necessary parameters, train quickly, and are particularly suited to high-dimensional data. Naive Bayes models calculate the probability of each class given the feature values, selecting the class with the highest probability as the prediction.
            </p> -->

            <p class="text-justified">
                Naive Bayes is a family of probabilistic classifiers based on applying Bayes' theorem with a "naive" assumption that features are conditionally independent given the class. Despite this simplifying assumption, Naive Bayes classifiers often perform surprisingly well in practice, especially for text classification, spam filtering, sentiment analysis, and recommendation systems. They require relatively small amounts of training data to estimate the necessary parameters, train quickly, and are particularly suited to high-dimensional data. Naive Bayes models calculate the probability of each class given the feature values, selecting the class with the highest probability as the prediction.
            </p>
            <p class="text-justified">
                The core idea is to flip the problem using Bayes' Theorem: instead of modeling the probability of features given a class being true (which can be hard), Naive Bayes models the likelihood of a class given the features. This makes it incredibly efficient, especially when combined with vectorized feature representations. Even when the independence assumption doesn't strictly hold, the model still tends to perform well in practice — a phenomenon sometimes called the "Naive Bayes miracle".
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/naive_bayes.jpg" alt="Naive Bayes Concept">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This diagram illustrates the fundamental concept of Naive Bayes classification. The model calculates the probability of each class based on observed features, making the "naive" assumption that features are conditionally independent given the class. This simplification allows for efficient computation even with many features.
                    </p>
                </div>
            </div>
            <h3 class="text-centered">Smoothing in Naive Bayes</h3>
            <!-- <p class="text-justified">
                One challenge with Naive Bayes is handling zero probabilities when a feature value doesn't appear in the training data for a given class. This can lead to the entire probability estimate for that class becoming zero due to multiplication. To address this, smoothing techniques like Laplace smoothing are used. These methods add a small constant to observed counts, ensuring that no probability is ever exactly zero. Smoothing helps improve generalization and ensures that the model can make predictions even for previously unseen feature values.
            </p> -->
            <p class="text-justified">
                One challenge with Naive Bayes is handling zero probabilities when a feature value doesn't appear in the training data for a given class. This can lead to the entire probability estimate for that class becoming zero due to multiplication. To address this, smoothing techniques like Laplace smoothing are used. These methods add a small constant to observed counts, ensuring that no probability is ever exactly zero. Smoothing helps improve generalization and ensures that the model can make predictions even for previously unseen feature values.
            </p>
            <p class="text-justified">
                Without smoothing, even a single missing word or unseen feature could dominate the entire prediction, which can be disastrous in real-world applications. Laplace (add-one) smoothing or Lidstone (add-α) smoothing are common techniques that mitigate this, especially in domains like text classification where many features are sparse.
            </p>

            <h3 class="text-centered">Different Types of Naive Bayes Classifiers</h3>
            <p class="text-justified">
                Naive Bayes' strength lies in its simplicity, computational efficiency, and effectiveness in many real-world scenarios. It excels when the independence assumption approximately holds or when the exact probability estimates are less important than the class rankings they produce. Different variants of Naive Bayes are suitable for different types of data:
            </p>
            <div class="two-column-container">
                <div class="method-card">
                    <h3 class="text-centered">Multinomial Naive Bayes</h3>
                    <p class="text-justified">
                        Suited for discrete counts, such as word frequencies in text classification. Features represent the frequencies with which certain events occur. Commonly used for document classification and spam filtering where feature vectors represent term frequencies.
                    </p>
                </div>
                <div class="method-card">
                    <h3 class="text-centered">Gaussian Naive Bayes</h3>
                    <p class="text-justified">
                        Appropriate for continuous data where features follow a normal distribution. Each class is modeled with a Gaussian distribution defined by mean and variance. Effective for classifying measurements like height, weight, or audio spectral features.
                    </p>
                </div>
                <div class="method-card">
                    <h3 class="text-centered">Bernoulli Naive Bayes</h3>
                    <p class="text-justified">
                        Designed for binary/boolean features. Models presence or absence of features rather than frequencies. Useful for text classification with binary word occurrence (present/absent) rather than counts, and in some cases of spam detection.
                    </p>
                </div>
                <div class="method-card">
                    <h3 class="text-centered">Categorical Naive Bayes</h3>
                    <p class="text-justified">
                        Handles categorical features directly without converting to numeric values. Each feature has its own categorical distribution per class. Particularly useful for nominal data where features have discrete, non-ordered categories.
                    </p>
                </div>
            </div>

            <p class="text-justified">
                The main differences between these variants lie in their assumptions about feature distributions. Multinomial assumes features follow a multinomial distribution suitable for count data. Gaussian works with continuous values assuming normal distributions. Bernoulli is optimal for binary features. Categorical handles nominal features without forcing numeric conversion. The appropriate variant should be selected based on your data characteristics for best performance.
            </p>

            <h3 class="text-centered">Naive Bayes Classification of Audio Data</h2>

            <p class="text-justified">
                To apply Naive Bayes classification to audio data, a dataset consisting of extracted features from six sound categories: piano, guitar, drums, thunder sounds, rain sounds, and wind sounds, is selected. 
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/data_for_naive_bayes_raw.png" alt="Raw Dataset View">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The dataset contains all the numeric audio features such as MFCCs, chroma, spectral contrast, tonnetz, and others, extracted for each audio file.
                    </p>
                </div>
            </div>

            <p class="text-justified">
                The raw data is grouped into two classes: <strong>music</strong> for records of categories piano, guitar, drums and <strong>outdoor</strong> for records of categories thunder, rain, wind.
                This transformation helps in converting a multi-class problem into a binary classification task, simplifying model training and evaluation.
                The categorized dataset is shown below.
            </p>
                
            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/data_for_naive_bayes_categorized.png" alt="Categorization into Music and Outdoor">
                </div>
            </div>

            <p class="text-justified">
                After categorization, the label column is encoded into binary form: music = 1 and outdoor = 0. This becomes the target variable for all the naive bayes classification models.
                The dataset after this transformation is shown below.
            </p>
            
            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/data_for_naive_bayes_categorized_encoded.png" alt="Binary Encoding of Labels">
                </div>
            </div>

            <p class="text-justified">
                All supervised learning methods require splitting data into training and testing sets to evaluate model performance objectively. This ensures that models are evaluated on data they haven't seen during training, providing a reliable estimate of how well they'll perform on new, unseen data.
                The dataset is split into training and testing sets in a 7:3 ratio. This means 70% of the dataset is used for training the models, and 30% is used for testing them.
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/gaussian_nb_train_data.png" alt="Train data">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This images depicts the training data, which is split from the original dataset, used for training the model. 
                        It contains 70% of the original dataset.
                    </p>
                </div>

                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/gaussian_nb_test_data.png" alt="Test data">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This images depicts the testing data on which the model is tested. It contains 30% of the original dataset.
                    </p>
                </div>
            </div>

            <p class="text-justified">
                Three variants of Naive Bayes classifiers are used: <strong>MultinomialNB</strong>, <strong>GaussianNB</strong>, and <strong>BernoulliNB</strong>. Each model has different feature format requirements. The training and testing datasets are transformed according to the models.
            </p>

            <h3 class="text-centered">Performing Multinomial Naive Bayes Classification</h3>

            <p class="text-justified">
                Multinomial Naive Bayes expects discrete feature values. To prepare the data, MinMaxScaler is first applied to scale features between 0 and 1. The scaled values are then multiplied by 100 and rounded to integers.
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/multinomial_nb_train_data_scaled.png" alt="Scaled Training Data - Multinomial">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This images depicts the scaled training data. The features are scaled between 0 and 1 using MinMaxScaler.
                    </p>
                </div>

                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/multinomial_nb_test_data_scaled.png" alt="Scaled Testing Data - Multinomial">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This images depicts the scaled testing data. The features are scaled between 0 and 1 using the parameters calculated on the training data.
                    </p>
                </div>
            </div>

            <p class="text-justified">
                To make the features discrete numbers, the scaled values are then multiplied by 100 and converted to integers by rounding them off.
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/multinomial_nb_train_data_discretized.png" alt="Discretized Training Data - Multinomial">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This images depicts the discretized training data. The scaled feature values are now integers between 0 and 100.
                    </p>
                </div>

                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/multinomial_nb_test_data_discretized.png" alt="Discretized Testing Data - Multinomial">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This images depicts the discretized testing data. The scaled feature values are now integers between 0 and 100.
                    </p>
                </div>
            </div>

            <h3 class="text-centered">Multinomial Naive Bayes Classification Results</h3>

            <p class="text-justified">
                The trained multinomial Naive Bayes model's predictions are tested against the testing data. 
                The performance of the model is evaluated using confusion matrix, classification heatmap, and ROC curve.
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/naive_bayes/multinomial_nb_classification_report_heatmap.png" alt="Classification Report - Multinomial NB">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The heatmap shows how well the model performs on both classes: outdoor and music. 
                        For class 0 (outdoor), the model achieved a precision of 0.837, recall of 0.888, and F1-score of 0.862. 
                        For class 1 (music), the precision was 0.876, recall was 0.820, and F1-score was 0.847. 
                        The overall accuracy of the model is 85.5%, and the macro averages for precision, recall, and F1-score are all around 0.85, suggesting that the performance is fairly balanced across both classes.
                    </p>
                </div>

                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/naive_bayes/multinomial_nb_confusion_matrix.png" alt="Confusion Matrix - Multinomial NB">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The confusion matrix provides the raw breakdown of predictions. 
                        The model correctly predicted 611 outdoor sounds and 543 music sounds. 
                        However, it also misclassified 77 outdoor sounds as music and 119 music sounds as outdoor. 
                        While the correct predictions outweigh the errors, the misclassification count—especially for music—shows that there’s still room for improvement. 
                        The total number of predictions is 1,350, out of which 1,154 were correct.
                    </p>
                </div>

                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/naive_bayes/multinomial_nb_roc_curve.png" alt="ROC Curve - Multinomial NB">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The ROC curve visualizes how well the model can distinguish between the two classes across different thresholds.
                        The curve leans toward the top-left corner, and the Area Under the Curve (AUC) is 0.91, which is quite good. 
                        It shows that the model has a strong ability to separate outdoor and music sounds, even if it isn’t perfect. 
                        AUC above 0.9 is generally considered very solid for a classifier like this.
                    </p>
                </div>
            </div>

            <h3 class="text-centered">Performing Gaussian Naive Bayes Classification</h3>

            <p class="text-justified">
                Gaussian Naive Bayes assumes normally distributed features. 
                Therefore, features are standardized using StandardScaler to have zero mean and unit variance.
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/gaussian_nb_train_data_scaled.png" alt="Scaled Training Data - Gaussian">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This images depicts the scaled training data. The features are scaled to a standard normal distribution with mean=0 and variance=1.
                    </p>
                </div>

                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/gaussian_nb_test_data_scaled.png" alt="Scaled Testing Data - Gaussian">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This images depicts the scaled testing data. The features are scaled to a standard normal distribution using parameters calculated on the training data.
                    </p>
                </div>
            </div>

            <h3 class="text-centered">Gaussian Naive Bayes Classification Results</h3>

            <p class="text-justified">
                The trained Gaussian Naive Bayes model's predictions are tested against the testing data. 
                The performance is evaluated using confusion matrix, classification heatmap, and ROC curve.
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/naive_bayes/gaussian_nb_classification_report_heatmap.png" alt="Classification Report - Gaussian NB">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The heatmap highlights the precision, recall, and F1-scores for both outdoor and music classes.
                        For class 0 (outdoor), the precision is 0.832, recall is 0.890, and F1-score is 0.860.
                        For class 1 (music), the precision is 0.876, recall is 0.813, and F1-score is 0.843.
                        These scores indicate the model is relatively balanced in how it handles both classes.
                        The overall accuracy stands at 85.2%, and the macro averages for precision, recall, and F1-score are all close to 0.85, showing consistently decent performance across the board.
                    </p>
                </div>

                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/naive_bayes/gaussian_nb_confusion_matrix.png" alt="Confusion Matrix - Gaussian NB">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The confusion matrix shows that the model correctly predicted 612 outdoor sounds and 538 music sounds.
                        There were 76 outdoor sounds misclassified as music, and 124 music sounds predicted as outdoor.
                        Out of 1,350 total predictions, 1,150 were accurate.
                        The diagonal dominance in the matrix confirms the model is doing a good job, but there’s still a moderate level of confusion between the two classes.
                    </p>
                </div>

                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/naive_bayes/gaussian_nb_roc_curve.png" alt="ROC Curve - Gaussian NB">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The ROC curve shows the trade-off between true positive rate and false positive rate.
                        The curve for this model rises sharply and stays close to the top-left corner, indicating strong classification ability.
                        The Area Under the Curve (AUC) is 0.922, which means the Gaussian Naive Bayes model is quite effective at separating the two classes overall.
                    </p>
                </div>
            </div>


            <h3 class="text-centered">Bernoulli Gaussian Naive Bayes Classification</h3>

            <p class="text-justified">
                Bernoulli Naive Bayes requires binary features. 
                Each feature value is binarized: if greater than the mean of that feature, it is set to 1; otherwise, it is set to 0.
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/bernoulli_nb_train_data.png" alt="Binary Training Data - Bernoulli">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This images depicts the binarized training data. The features are encoded to have values of either 1 or 0.
                    </p>
                </div>

                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/bernoulli_nb_test_data.png" alt="Binary Testing Data - Bernoulli">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This images depicts the binarized testing data. The features are encoded to have values of either 1 or 0.
                    </p>
                </div>
            </div>

            <h3 class="text-centered">Bernoulli Naive Bayes Classification Results</h3>

            <p class="text-justified">
                The trained Bernoulli Naive Bayes model's predictions are tested against the testing data. 
                The performance is evaluated using confusion matrix, classification heatmap, and ROC curve.
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/naive_bayes/bernoulli_nb_classification_report_heatmap.png" alt="Classification Report - Bernoulli NB">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The heatmap shows the model’s classification metrics for both outdoor and music classes.
                        For class 0 (outdoor), the precision is 0.869, recall is 0.850, and F1-score is 0.860.
                        For class 1 (music), the precision is 0.848, recall is 0.867, and F1-score is 0.857.
                        The overall accuracy of the model is 85.9%, and the macro average precision, recall, and F1-scores are all close to 0.86.
                        These values suggest a well-balanced performance, with both classes being handled nearly equally well.
                    </p>
                </div>

                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/naive_bayes/bernoulli_nb_confusion_matrix.png" alt="Confusion Matrix - Bernoulli NB">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The confusion matrix reveals that the model correctly predicted 585 outdoor sounds and 574 music sounds.
                        It misclassified 103 outdoor samples as music and 88 music samples as outdoor.
                        This totals 1,159 correct predictions out of 1,350.
                        The error rate is fairly low, and the matrix shows the model is reliable for both categories.
                    </p>
                </div>

                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/naive_bayes/bernoulli_nb_roc_curve.png" alt="ROC Curve - Bernoulli NB">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The ROC curve illustrates the model’s ability to distinguish between classes across thresholds.
                        The curve is steep and hugs the top-left corner, indicating excellent separation performance.
                        The Area Under the Curve (AUC) is 0.937, showing that the Bernoulli Naive Bayes model is highly effective at classifying between outdoor and music sounds.
                    </p>
                </div>
            </div>
            
            <h3 class="text-centered">Conclusion</h3>

            <p class="text-justified">
                Naive Bayes classifiers performed consistently well in distinguishing between outdoor and music sounds using audio features. 
                All three variants: Multinomial, Gaussian, and Bernoulli, achieved accuracy around 85%, with Bernoulli Naive Bayes performing the best overall. It had the highest F1-score balance between classes and the highest AUC of 0.937, indicating strong classification confidence.
            </p>
            <p class="text-justified">
                These results suggest that even simple probabilistic models like Naive Bayes can handle audio feature-based classification tasks effectively when features are preprocessed appropriately. The project revealed that audio categories like music and outdoor sounds have distinguishable statistical patterns, which Naive Bayes models are able to capture despite their assumptions of feature independence.
            </p>
            <p class="text-justified">
                The full script to perform Naive Bayes Classification with its corresponding preprocessing and visualizations can be found 
                <a href="https://github.com/AnirudhKakati/sound-wave-analysis/blob/main/scripts/naive_bayes.py" target="_blank">here.</a>
            </p>


        </div>
        <!-- End of Naive Bayes Section -->

        <!-- Decision Trees Section -->
        <div id="decision-trees" class="content-section" data-aos="fade-up">
            <h2>Decision Trees</h2>
            
            <h3 class="text-centered">Overview</h3>
            <p class="text-justified">
                Decision Trees are versatile supervised learning algorithms that create a model resembling a tree-like structure of decisions. Each internal node represents a "test" on a feature, each branch represents the outcome of the test, and each leaf node represents a class label or a value prediction. 
                Decision Trees are intuitive, easy to interpret, and can handle both classification and regression tasks, making them popular across various domains including finance, healthcare, and computer vision.
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/decision_tree.png" alt="Decision Tree Concept">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This visualization shows a simple decision tree with 4 features. Starting from the root node (top), the algorithm makes decisions based on features, following different paths until reaching a leaf node that represents a decision category. Each internal node splits the data based on a feature threshold that optimally separates the classes.
                    </p>
                </div>
            </div>

            <p class="text-justified">
                Decision Trees work by recursively partitioning the feature space into regions, attempting to find the splits that create the most homogeneous groups with respect to the target variable. To determine the best splits, various impurity measures are used:
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/gini_vs_entropy.jpeg" alt="Gini and Entropy Comparison">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This chart compares Gini Impurity and Entropy, the two most common measures for evaluating splits in classification trees. Both measure class mixing, with lower values indicating better splits. Gini tends to be computationally simpler, while Entropy can sometimes produce more balanced trees. Both reach minimum (0) for pure nodes and maximum for equally mixed nodes.
                    </p>
                </div>
            </div>

            <h3 class="text-centered">Impurity Measures and Information Gain</h4>
            <p class="text-justified">
                The quality of a split in a Decision Tree is determined by how much it reduces impurity. This reduction is quantified using Information Gain, which measures the difference in entropy (or Gini impurity) before and after a split. Let's understand these concepts with a simple example:
            </p>

        
            <div class="method-card">
                <p class="text-justified">
                  Consider a synthesized dataset with 12 audio samples from three categories: "rain", "fireworks", and "birds".
                  Each sample has a feature "zero_crossing_rate".
                </p>
              
                <ul class="ul-list">
                  <li>Rain: 4 samples</li>
                  <li>Fireworks: 4 samples</li>
                  <li>Birds: 4 samples</li>
                </ul>
              
                <p class="text-justified">
                  We want to determine whether to split the data based on the feature "zero_crossing_rate" with a threshold of 0.5.
                </p>
              
                <p class="text-justified">
                  First, we calculate the entropy of the parent node:
                </p>
              
                <div class="equation">
                  Entropy(parent) = -(4/12 * log₂(4/12) + 4/12 * log₂(4/12) + 4/12 * log₂(4/12)) = 1.58
                </div>
              
                <p class="text-justified">
                  After splitting on "zero_crossing_rate > 0.5", we get:
                </p>
              
                <ul class="ul-list">
                  <li>Left child (zero_crossing_rate ≤ 0.5): 6 samples – 4 "rain", 2 "birds"</li>
                  <li>Right child (zero_crossing_rate > 0.5): 6 samples – 4 "fireworks", 2 "birds"</li>
                </ul>
              
                <p class="text-justified">
                  Now we calculate the entropy of each child node:
                </p>
              
                <div class="equation">
                  Entropy(left) = -(4/6 * log₂(4/6) + 2/6 * log₂(2/6)) = 0.92
                </div>
              
                <div class="equation">
                  Entropy(right) = -(4/6 * log₂(4/6) + 2/6 * log₂(2/6)) = 0.92
                </div>
              
                <p class="text-justified">
                  The weighted average entropy after the split is:
                </p>
              
                <div class="equation">
                  Entropy(after split) = (6/12 * 0.92) + (6/12 * 0.92) = 0.92
                </div>
              
                <p class="text-justified">
                  Finally, we calculate the Information Gain:
                </p>
              
                <div class="equation">
                  Information Gain = Entropy(parent) - Entropy(after split) = 1.58 - 0.92 = 0.66
                </div>
              
                <p class="text-justified">
                  This positive Information Gain of 0.66 indicates that splitting on "zero_crossing_rate > 0.5" is effective in reducing impurity and is a strong candidate for decision making in our tree. The decision tree algorithm would compare this gain with other feature splits and choose the one with the highest gain.
                </p>
            </div>

            <p class="text-justified">
                It's worth noting that an infinite number of decision trees can be created for the same dataset by varying:
            </p>
            <ul class="ul-list">
                <li>The choice of features to split on</li>
                <li>The thresholds for each split</li>
                <li>The order of splits (which feature to use first, second, etc.)</li>
                <li>The depth of the tree</li>
                <li>Pruning strategies to avoid overfitting</li>
            </ul>
            <p class="text-justified">
                This flexibility is both a strength and a challenge, as it allows for highly customized trees but requires careful tuning to avoid overfitting or creating unnecessarily complex models.
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/decision_trees_varying_complexity.png" alt="Decision Tree Complexity">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This visualization shows trees of varying complexity: a simple tree with few splits (left), a balanced tree with moderate complexity (middle), and an overfit tree with many splits (right). The optimal tree complexity balances model simplicity with predictive power, typically determined through pruning or setting maximum depth constraints.
                    </p>
                </div>
            </div>

            <h3 class="text-centered">Decision Tree Classification of Audio Data</h2>
            <p class="text-justified">
                For Decision Tree classification of audio data, we specifically use audio data from two categories: <strong>birds</strong> and <strong>dog_bark</strong>. The dataset is shown below.
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/decision_tree_raw_data.png" alt="Decision Tree Dataset">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The dataset used for Decision Tree analysis includes all 43 acoustic features for the two categories birds and dog_bark. 
                    </p>
                </div>
            </div>

            <p class="text-justified">
                The label column is encoded into binary form: birds = 1 and dog_bark = 0. This becomes the target variable for the decision tree classification models. The dataset after this transformation is shown below.
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/decision_tree_raw_data_encoded.png" alt="Decision Tree Dataset Encoded">
                </div>
            </div>


            <p class="text-justified">
                The same fundamental approach as with Naive Bayes is used: splitting the dataset into training and testing sets with 70% used for training and 30% used for testing sets using stratified sampling. This train-test split is crucial for objective evaluation of model performance and remains constant across all supervised learning methods for fair comparison. However, Decision Trees have different preprocessing requirements, as they can work with both categorical and numerical features without assumptions about feature distributions. 
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/decision_tree_train_data.png" alt="Decision Tree Training data">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The image shows the training dataset for the decision tree classifiers. It contains 70% of the original dataset.
                    </p>
                </div>

                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/decision_tree_test_data.png" alt="Decision Tree Testing data">
                </div>

                <div class="plot-description">
                    <p class="text-justified">
                        The image shows the testing dataset for the decision tree classifiers. It contains 30% of the original dataset.
                    </p>
                </div>
            </div>

            <p class="text-justified">
                Decision Trees can naturally handle continuous features by finding optimal thresholds for splits. 
                The only preprocessing applied was standardization (mean=0, std=1) to ensure features with larger scales don't dominate those with smaller scales during the initial split evaluations.
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/decision_tree_train_data_scaled.png" alt="Decision Tree Training data scaled">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The image shows the scaled training dataset for the decision tree classifiers. 
                        The features are scaled to a standard normal distribution with mean=0 and variance=1.
                    </p>
                </div>

                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/decision_tree_test_data_scaled.png" alt="Decision Tree Testing data">
                </div>

                <div class="plot-description">
                    <p class="text-justified">
                        The image shows the scaled testing dataset for the decision tree classifiers. 
                        The features are scaled to a standard normal distribution using parameters calculated on the training data.
                    </p>
                </div>
            </div>

            <h3 class="text-centered">Implementing Decision Trees with Varying Parameters</h3>

            <p class="text-justified">
                The implementation of Decision Trees for audio classification uses scikit-learn's <code>DecisionTreeClassifier</code>. Three different trees were created, each with unique hyperparameters to explore how variations in depth, feature selection, and splitting criteria influence the structure and decision paths of the trees:
                <ul class="ul-list">
                    <li>Tree 1: Uses the Gini impurity criterion, with a maximum depth of 3 and a maximum of 10 features considered.</li>
                    <li>Tree 2: Uses the log loss (cross-entropy) criterion, with a maximum depth of 2 and only 5 features considered.</li>
                    <li>Tree 3: Uses the entropy criterion, with a maximum depth of 3 and no restriction on the number of features.</li>
                </ul>
            </p>
            <p class="text-justified">
                The three Decision Tree models were evaluated on the audio classification task, each revealing different aspects of tree-based learning.
            </p>

            <h3 class="text-centered">Tree 1 Classification Results</h3>

            <p class="text-justified">
                The first tree model's predictions are tested against the testing data. The performance is evaluated using confusion matrix, classification heatmap, and ROC curve.
            </p>


            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/decision_trees/tree_1_tree_plot.png" alt="Tree Plot - Tree 1">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The decision tree uses Gini impurity to classify audio as "birds" or "dog_bark" with a maximum depth of 3 and 10 features. 
                        The root node splits on mfcc_1 (threshold 0.125) with 1050 total samples. Key features include zero_crossing_rate (0.217) leading to dog_bark classifications and spectral_bandwidth (0.381) typically leading to birds classifications. 
                        Terminal nodes show good separation with low Gini values, demonstrating that acoustic features effectively distinguish between the two sound types.
                    </p>
                </div>

                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/decision_trees/tree_1_classification_report.png" alt="Classification Report - Tree 1">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The heatmap summarizes how well the decision tree model performs across the two classes: dog_bark and birds. 
                        For class 0 (dog_bark), the precision is 0.887, recall is 0.842, and F1-score is 0.864. 
                        For class 1 (birds), the precision is 0.838, recall is 0.884, and F1-score is 0.860. 
                        The overall accuracy of the model is 86.2%, and the macro averages for precision, recall, and F1-score are all close to 0.86, indicating balanced classification performance across both classes.
                    </p>
                </div>

                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/decision_trees/tree_1_confusion_matrix.png" alt="Confusion Matrix - Tree 1">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The confusion matrix provides a detailed breakdown of the model’s predictions. 
                        The model correctly predicted 197 dog_bark sounds and 191 bird sounds. 
                        However, it misclassified 37 dog_bark sounds as birds and 25 bird sounds as dog_bark. 
                        While the majority of predictions were accurate, the misclassifications indicate areas where the model could still improve.
                    </p>
                </div>

                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/decision_trees/tree_1_roc_curve.png" alt="ROC Curve - Tree 1">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The ROC curve shows the model’s ability to distinguish between the two classes across various threshold values. 
                        The curve trends toward the top-left corner, and the Area Under the Curve (AUC) is 0.914. 
                        This high AUC score reflects the model’s strong capacity to separate dog_bark and bird sounds effectively.
                    </p>
                </div>
            </div>

            <h3 class="text-centered">Tree 2 Classification Results</h3>

            <p class="text-justified">
                Next, the second tree model's predictions are tested against the testing data. The performance is evaluated using confusion matrix, classification heatmap, and ROC curve.
            </p>


            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/decision_trees/tree_2_tree_plot.png" alt="Tree Plot - Tree 1">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The decision tree uses LogLoss criterion with a maximum depth of 2 and considers only 5 features. 
                        The root node splits on chroma_5 (threshold 0.034) with 1050 total samples. 
                        When chroma_5 ≤ 0.034, it further splits on mfcc_2 (threshold -1.108) leading primarily to dog_bark classifications. 
                        When chroma_5 > 0.034, samples maintain birds classification with most leaf nodes showing decreasing log_loss values. 
                        The model efficiently categorizes audio samples using minimal depth while achieving reasonable separation between bird sounds and dog barks.
                    </p>
                </div>

                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/decision_trees/tree_2_classification_report.png" alt="Classification Report - Tree 1">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The heatmap shows the performance of the decision tree model (Tree 2) across the two classes: dog_bark and birds. 
                        For class 0 (dog_bark), the model achieved a precision of 0.847, recall of 0.756, and F1-score of 0.799. 
                        For class 1 (birds), the precision was 0.763, recall was 0.852, and F1-score was 0.805. 
                        The overall accuracy of the model is 80.2%, and the macro-averaged precision, recall, and F1-score are all close to 0.80, indicating balanced but slightly reduced performance compared to deeper trees.
                    </p>
                </div>

                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/decision_trees/tree_2_confusion_matrix.png" alt="Confusion Matrix - Tree 1">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The confusion matrix shows how the model’s predictions are distributed. 
                        The model correctly identified 177 dog_bark sounds and 184 bird sounds. 
                        However, 57 dog_bark samples were misclassified as birds, while 32 bird sounds were labeled as dog_bark. 
                        While the model shows reasonable accuracy, the higher misclassification of dog_bark samples suggests room for improvement.
                    </p>
                </div>

                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/decision_trees/tree_2_roc_curve.png" alt="ROC Curve - Tree 1">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The ROC curve illustrates the trade-off between the true positive rate and false positive rate at various thresholds. 
                        For this model, the curve has a noticeable upward bend but doesn’t sharply hug the top-left corner, and the Area Under the Curve (AUC) is 0.827. 
                        This AUC value suggests that while the model can distinguish between classes reasonably well, its discriminative ability is slightly lower than Tree 1.
                    </p>
                </div>
            </div>

            <h3 class="text-centered">Tree 3 Classification Results</h3>

            <p class="text-justified">
                Finally, the third tree model's predictions are tested against the testing data. The performance is evaluated using confusion matrix, classification heatmap, and ROC curve.
            </p>


            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/decision_trees/tree_3_tree_plot.png" alt="Tree Plot - Tree 1">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The decision tree uses Entropy criterion with a maximum depth of 3 and no feature restrictions. 
                        The root node splits on spectral_contrast_7 (threshold 0.213) with 1050 total samples. 
                        Key features include mfcc_4 (threshold -0.228) leading to dog_bark classifications and spectral_contrast_3 (threshold -0.541) typically resulting in birds classifications. 
                        Several terminal nodes achieve perfect separation (entropy = 0.0), particularly in the bird classification branches. 
                        The tree effectively leverages acoustic spectral features and MFCCs to distinguish between the two sound types. 
                    </p>
                </div>

                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/decision_trees/tree_3_classification_report.png" alt="Classification Report - Tree 1">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The heatmap summarizes the classification performance of the decision tree model (Tree 3) across dog_bark and bird sounds. 
                        For class 0 (dog_bark), the model achieved a precision of 0.868, recall of 0.868, and F1-score of 0.868. 
                        For class 1 (birds), the precision was 0.856, recall was 0.856, and F1-score was 0.856. 
                        The overall accuracy of the model is 86.2%, and the macro averages for precision, recall, and F1-score are all 0.862, indicating consistent and balanced performance between the two classes.
                    </p>
                </div>

                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/decision_trees/tree_3_confusion_matrix.png" alt="Confusion Matrix - Tree 1">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The confusion matrix shows that the model correctly classified 203 dog_bark sounds and 185 bird sounds. 
                        However, it also misclassified 31 samples from each class, with dog_bark sounds predicted as birds and vice versa. 
                        Despite these misclassifications, the model made 388 correct predictions out of 450 total, reflecting strong reliability in identifying both categories.
                    </p>
                </div>

                <div class="plot-container" data-aos="fade-up">
                    <img src="plots/decision_trees/tree_3_roc_curve.png" alt="ROC Curve - Tree 1">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The ROC curve highlights the model’s strong ability to differentiate between the two classes. 
                        The curve hugs the top-left corner closely, and the Area Under the Curve (AUC) is 0.937, the highest among the three trees. 
                        This indicates excellent discriminative performance, with the model maintaining high sensitivity and specificity across thresholds.
                    </p>
                </div>
            </div>

            <h3 class="text-centered">Conclusion</h3>

            <p class="text-justified">
                Decision Tree classifiers demonstrated solid performance in classifying between bird and dog_bark sounds based on extracted audio features. 
                All three trees—using Gini, LogLoss, and Entropy as splitting criteria—achieved accuracy between 80% and 86%, with Tree 3 (Entropy, depth=3) performing the best overall. 
                It had the most balanced precision and recall across classes and the highest AUC of 0.937, indicating excellent class separation capability.
            </p>
            <p class="text-justified">
                These results highlight the interpretability and flexibility of decision trees, especially when working with structured feature sets. 
                Even shallow trees with limited depth were able to capture meaningful distinctions between sound types, showing that hierarchical rule-based models can be effective for sound classification when the features are informative.
            </p>
            <p class="text-justified">
                The full script to run Decision Tree Classification along with preprocessing and evaluation visualizations is available 
                <a href="https://github.com/AnirudhKakati/sound-wave-analysis/blob/main/scripts/decision_tree.py" target="_blank">here.</a>
            </p>
        </div>
        <!-- End of Decision Trees Section -->

        <!-- Regression Section -->
        <div id="regression" class="content-section" data-aos="fade-up">
            <h2>Regression</h2>
            
            <h3 class="text-centered">Understanding Regression Models</h3>
            
            <h4 class="text-centered">Linear Regression</h4>
            <p class="text-justified">
                Linear regression is a supervised learning algorithm that models the relationship between a dependent variable and one or more independent variables by fitting a linear equation to the observed data. The model assumes a linear relationship where the output is a weighted sum of the input features plus a bias term. Linear regression is primarily used for predicting continuous values and estimating the strength of relationships between variables. Its simplicity, interpretability, and computational efficiency make it a foundational technique in statistics and machine learning.
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/linear_regression.png" alt="Linear Regression Visualization">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This visualization shows a simple linear regression with one input feature. The blue points represent data samples, and the red line represents the linear model that minimizes the sum of squared differences between predicted and actual values. The equation of the line is typically expressed as y = β₀ + β₁x, where β₀ is the intercept and β₁ is the slope.
                    </p>
                </div>
            </div>

            <h4 class="text-centered">Logistic Regression</h4>
            <p class="text-justified">
                Logistic regression is a supervised learning algorithm used for binary classification problems. Despite its name, it's a classification algorithm rather than a regression technique. Logistic regression models the probability that an instance belongs to a particular class using the logistic function (sigmoid) to transform a linear combination of features into a value between 0 and 1. This probability can then be thresholded to make binary predictions. Logistic regression is widely used in fields like medicine, marketing, and risk assessment for its interpretable results and probability estimates.
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/logistic_regression.png" alt="Logistic Regression Visualization">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This visualization illustrates logistic regression for binary classification. The colored points represent data from two classes, and the curved line represents the decision boundary where the predicted probability equals 0.5. The S-shaped curve shows how the linear model is transformed by the sigmoid function to produce probabilities between 0 and 1.
                    </p>
                </div>
            </div>

            <h4 class="text-centered">Similarities and Differences</h4>
            <p class="text-justified">
                Linear and logistic regression share several similarities but serve fundamentally different purposes:
            </p>

            <div class="comparison-table">
                <table>
                    <tr>
                        <th>Aspect</th>
                        <th>Linear Regression</th>
                        <th>Logistic Regression</th>
                    </tr>
                    <tr>
                        <td>Purpose</td>
                        <td>Predicts continuous numerical values</td>
                        <td>Predicts probabilities for classification</td>
                    </tr>
                    <tr>
                        <td>Output Range</td>
                        <td>Unbounded (any real number)</td>
                        <td>Bounded between 0 and 1</td>
                    </tr>
                    <tr>
                        <td>Transformation Function</td>
                        <td>None (linear combination of inputs)</td>
                        <td>Sigmoid function</td>
                    </tr>
                    <tr>
                        <td>Loss Function</td>
                        <td>Mean Squared Error</td>
                        <td>Cross-Entropy Loss</td>
                    </tr>
                    <tr>
                        <td>Optimization Method</td>
                        <td>Closed-form solution or gradient descent</td>
                        <td>Typically gradient descent</td>
                    </tr>
                    <tr>
                        <td>Interpretability</td>
                        <td>Coefficients represent change in output per unit change in input</td>
                        <td>Coefficients represent log-odds ratios</td>
                    </tr>
                </table>
            </div>

            <p class="text-justified">
                Both models are linear in their parameters, making them interpretable and efficient to train. However, they differ significantly in their applications, with linear regression focusing on value prediction and logistic regression on class probability estimation. Their different output ranges and loss functions reflect these distinct purposes.
            </p>

            <h4 class="text-centered">The Sigmoid Function in Logistic Regression</h4>
            <p class="text-justified">
                Yes, logistic regression uses the sigmoid function (also called the logistic function) as its key component. The sigmoid function transforms the linear combination of input features into a probability value between 0 and 1. It has an S-shaped curve defined by the equation σ(z) = 1/(1+e^(-z)), where z is the linear combination of features. The sigmoid function solves a fundamental problem: while linear regression outputs can range from negative infinity to positive infinity, probabilities must be bounded between 0 and 1. The function's shape also creates a natural decision boundary at 0.5 probability.
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/sigmoid_function.png" alt="Sigmoid Function">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This graph shows the sigmoid function σ(z) = 1/(1+e^(-z)). The function approaches 0 as z goes to negative infinity and approaches 1 as z goes to positive infinity. At z=0, the function outputs exactly 0.5. This property creates a natural classification threshold where values above 0 are classified as the positive class and values below 0 as the negative class.
                    </p>
                </div>
            </div>

            <h4 class="text-centered">Maximum Likelihood and Logistic Regression</h4>
            <p class="text-justified">
                Maximum likelihood estimation (MLE) is the statistical principle underlying logistic regression training. While linear regression minimizes the sum of squared errors, logistic regression maximizes the likelihood of observing the given data under the model's probability distributions. The likelihood function measures how probable the observed data is given the current model parameters. For each data point, logistic regression computes the probability of the observed class, and the goal is to find parameter values that maximize the product of these probabilities (or, equivalently, the sum of log probabilities). This approach naturally leads to the cross-entropy loss function used in logistic regression and provides not just class predictions but well-calibrated probability estimates.
            </p>

            <h3 class="text-centered">Implementing Logistic Regression for Audio Classification</h3>
            <p class="text-justified">
                For our audio classification task, we implemented multinomial logistic regression (also known as softmax regression for multi-class problems) and compared it with Multinomial Naive Bayes. To create a binary classification problem as required, we selected two distinct sound categories: "dog_bark" and "sirens".
            </p>

            <div class="visualization-section">
                <h4 class="text-centered">Binary Dataset</h4>
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/binary_dataset.png" alt="Binary Classification Dataset">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The binary classification dataset contains samples from only two sound categories: "dog_bark" and "sirens". These categories were chosen because they represent distinctly different acoustic patterns—animal vocalizations versus mechanical warning sounds. All 43 acoustic features were retained for the classification task.
                    </p>
                </div>
            </div>

            <h3 class="text-centered">Results Comparison</h3>
            <p class="text-justified">
                Both logistic regression and Multinomial Naive Bayes were evaluated on the binary classification task:
            </p>

            <div class="visualization-section">
                <h4 class="text-centered">Performance Metrics</h4>
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/lr_nb_comparison.png" alt="Logistic Regression vs. Naive Bayes">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This comparison shows that Logistic Regression achieved 94.2% accuracy, outperforming Multinomial Naive Bayes at 89.7%. Logistic Regression also showed higher precision, recall, and F1-score for both classes. The superiority of Logistic Regression can be attributed to its ability to model feature interactions implicitly through the weight optimization process, while Naive Bayes's independence assumption limits its capacity to capture complex feature relationships.
                    </p>
                </div>
            </div>

            <div class="visualization-section">
                <h4 class="text-centered">Confusion Matrices</h4>
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/lr_nb_confusion.png" alt="Confusion Matrices Comparison">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The confusion matrices reveal that Logistic Regression (left) has fewer misclassifications than Multinomial Naive Bayes (right). Logistic Regression particularly excels at correctly identifying "dog_bark" samples, while Naive Bayes shows more balanced errors between the two classes. These differences reflect the distinct mathematical foundations of the two algorithms.
                    </p>
                </div>
            </div>

            <div class="visualization-section">
                <h4 class="text-centered">Probability Calibration</h4>
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/probability_calibration.png" alt="Probability Calibration">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This chart shows the probability distributions produced by both models for the test samples. Logistic Regression (blue) tends to produce more confident probability estimates, with most predictions clustered near 0 or 1. Naive Bayes (orange) generally produces more moderate probabilities, reflecting its different approach to probability estimation. Well-calibrated probabilities are important in applications where risk assessment is more important than just binary decisions.
                    </p>
                </div>
            </div>

            <h3 class="text-centered">Conclusion</h3>
            <p class="text-justified">
                Our comparison of logistic regression and Multinomial Naive Bayes for binary audio classification yields several insights:
            </p>
            
            <ul class="ul-list">
                <li>Logistic regression significantly outperformed Naive Bayes (94.2% vs. 89.7% accuracy) for distinguishing between "dog_bark" and "sirens" sounds, demonstrating its effectiveness for audio classification tasks.</li>
                <li>The performance advantage of logistic regression likely stems from its ability to model conditional dependencies between features, which is particularly relevant for audio data where spectral and temporal features are inherently related.</li>
                <li>Logistic regression produces more confident probability estimates, which can be beneficial in applications requiring decisive classifications but may need calibration for risk assessment scenarios.</li>
                <li>Both models provided interpretable coefficients that highlight which acoustic features are most discriminative between animal vocalizations and mechanical warning sounds.</li>
                <li>Preprocessing played a crucial role in both models' performance, with standardization being essential for logistic regression and non-negative scaling for Multinomial Naive Bayes.</li>
            </ul>
            
            <p class="text-justified">
                While our binary classification task was simplified compared to the full 20-class problem, it demonstrated the fundamental differences between generative (Naive Bayes) and discriminative (logistic regression) approaches to classification. Logistic regression's strong performance suggests it could be a valuable component in ensemble methods or as a benchmark for more complex models in audio classification tasks.
            </p>
        </div>

        <!-- Support Vector Machines Section
        <div id="svm" class="content-section" data-aos="fade-up">
            <h2>Support Vector Machines</h2>
            
            <h3 class="text-centered">Overview</h3>
            <p class="text-justified">
                Support Vector Machines (SVMs) are powerful supervised learning algorithms used for classification, regression, and outlier detection. SVMs are particularly effective in high-dimensional spaces and cases where the number of dimensions exceeds the number of samples. The core idea behind SVMs is to find the optimal hyperplane that maximizes the margin between different classes in the feature space. This margin maximization gives SVMs strong generalization properties, making them less prone to overfitting compared to many other classifiers.
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/svm_concept.png" alt="SVM Concept">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This illustration shows the fundamental concept of SVMs. The algorithm finds the hyperplane (solid line) that maximizes the margin between classes. The margin is the distance between the hyperplane and the closest data points from each class, known as support vectors (circled). These support vectors are the only data points that determine the hyperplane's position, making SVMs robust to outliers far from the decision boundary.
                    </p>
                </div>
            </div>

            <p class="text-justified">
                One of SVM's most powerful features is the kernel trick, which allows the algorithm to operate in an implicit higher-dimensional feature space without ever computing the coordinates of the data in that space. This enables SVMs to find nonlinear decision boundaries in the original feature space by mapping the data into a higher-dimensional space where a linear boundary can separate the classes.
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/kernel_trick.png" alt="Kernel Trick Visualization">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This visualization demonstrates the kernel trick. The left side shows data that's not linearly separable in 2D space. The middle shows how these points can be mapped to a higher-dimensional space (3D in this example) where they become linearly separable. The right side shows the resulting nonlinear decision boundary when projected back to the original 2D space. Common kernels include linear, polynomial, radial basis function (RBF), and sigmoid.
                    </p>
                </div>
            </div>
            
            <h3 class="text-centered">Data Preparation</h3>
            <p class="text-justified">
                SVMs are sensitive to feature scaling, so proper preprocessing is essential. For our audio classification task, we standardized all features to have zero mean and unit variance, which helps prevent features with larger scales from dominating the model. We maintained the same train-test split used for previous models to ensure fair comparison.
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/svm_data_prep.png" alt="SVM Data Preparation">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This image shows the effect of standardization on the feature distributions. The left side shows the original feature distributions with varying scales and centers. The right side shows the standardized features, all centered at zero with similar scales. This preprocessing is crucial for SVM performance, as the algorithm's distance calculations are directly affected by feature magnitudes.
                    </p>
                </div>
            </div>

            <h3 class="text-centered">SVM Implementation</h3>
            <p class="text-justified">
                We implemented SVM classification for our audio dataset using scikit-learn's SVC (Support Vector Classification) with different kernel functions. Due to the computational intensity of training SVMs on large datasets, we used a stratified subset of the data with 500 samples per class.
            </p>
            


            <p class="text-justified">
                The complete implementation with additional metrics, visualizations, and hyperparameter tuning can be found in the 
                <a href="https://github.com/AnirudhKakati/sound-wave-analysis/blob/main/scripts/svm.py" target="_blank">GitHub repository</a>.
            </p>

            <h3 class="text-centered">Results</h3>
            <p class="text-justified">
                We evaluated the performance of SVMs with different kernels on our audio classification task:
            </p>

            <div class="visualization-section">
                <h4 class="text-centered">Accuracy Comparison</h4>
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/svm_accuracy.png" alt="SVM Accuracy Comparison">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This chart compares the accuracy of SVM models with different kernels. The RBF kernel achieved the highest accuracy at 84.7%, followed by the polynomial kernel at 82.1%, and the linear kernel at 79.5%. The optimized RBF model after hyperparameter tuning reached 86.3%, demonstrating the importance of parameter selection in SVM performance.
                    </p>
                </div>
            </div>

            <div class="visualization-section">
                <h4 class="text-centered">Confusion Matrix - RBF Kernel</h4>
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/svm_confusion_matrix.png" alt="SVM Confusion Matrix">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        The confusion matrix for the RBF kernel SVM shows strong classification performance across most sound categories. The model excels at recognizing distinct sounds like "sirens" and "piano" with few confusions. Similar to other classifiers, there are some confusions between acoustically similar categories like "traffic_sounds" and "train_sounds". The RBF kernel's ability to capture complex, nonlinear relationships in the feature space contributes to its superior performance.
                    </p>
                </div>
            </div>

            <div class="visualization-section">
                <h4 class="text-centered">Decision Boundaries Visualization</h4>
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/svm_decision_boundaries.png" alt="SVM Decision Boundaries">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This visualization shows decision boundaries for different SVM kernels in a simplified 2D space using the first two principal components. The linear kernel (left) creates straight-line boundaries, while the polynomial (middle) and RBF (right) kernels create curved boundaries that can capture more complex relationships. The RBF kernel produces the most flexible decision boundary, allowing it to better separate the sound categories in this reduced feature space.
                    </p>
                </div>
            </div>

            <div class="visualization-section">
                <h4 class="text-centered">Hyperparameter Sensitivity</h4>
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/svm_hyperparameters.png" alt="SVM Hyperparameter Sensitivity">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This heatmap shows the impact of different C and gamma values on the RBF kernel SVM's accuracy. The regularization parameter C controls the trade-off between margin maximization and training error minimization, while gamma determines the influence radius of each support vector. Higher accuracy is observed with moderate C values (10-100) and lower gamma values (0.01-0.1), indicating that a moderately complex model with broader influence regions performs best for audio classification.
                    </p>
                </div>
            </div>

            <h3 class="text-centered">Conclusion</h3>
            <p class="text-justified">
                The SVM analysis yielded several important insights for audio classification:
            </p>
            
            <ul class="ul-list">
                <li>SVMs with the RBF kernel achieved the highest accuracy among all individual models tested (86.3% after optimization), demonstrating their effectiveness for audio classification tasks.</li>
                <li>The superior performance of nonlinear kernels (RBF and polynomial) compared to the linear kernel indicates that audio features have complex, nonlinear relationships that benefit from more flexible decision boundaries.</li>
                <li>Hyperparameter tuning significantly improved SVM performance, highlighting the importance of parameter selection, particularly for the regularization parameter C and the kernel parameter gamma.</li>
                <li>SVMs showed strong generalization ability, maintaining good performance across diverse sound categories despite being trained on a smaller subset of the data.</li>
                <li>The primary drawback of SVMs was their computational intensity, requiring significantly longer training times than Naive Bayes or Decision Trees, particularly for large datasets with many classes.</li>
            </ul>
            
            <p class="text-justified">
                SVMs provide a powerful approach for audio classification, particularly when accuracy is prioritized over training efficiency. Their ability to handle high-dimensional data with complex relationships makes them well-suited for capturing the nuanced spectral and temporal patterns in audio signals. While SVMs don't provide the same level of interpretability as Decision Trees, their strong performance and theoretical foundations in statistical learning theory make them a valuable tool in the audio classification toolkit.
            </p>
        </div> -->

        <!-- <div class="content-section" data-aos="fade-up">
            <h3 class="text-centered">Overall Comparison of Supervised Learning Methods</h3>
            <p class="text-justified">
                After evaluating four different supervised learning approaches on the audio classification task, we can compare their performance, strengths, and limitations:
            </p>

            <div class="visualization-section">
                <div class="plot-container" data-aos="fade-up">
                    <img src="images/supervised/overall_comparison.png" alt="Overall Comparison">
                </div>
                <div class="plot-description">
                    <p class="text-justified">
                        This chart compares the best-performing model from each approach. The optimized SVM with RBF kernel achieved the highest accuracy at 86.3%, followed by Decision Trees at 82.3%, Logistic Regression at 78.9% (on the full multi-class problem), and Gaussian Naive Bayes at 76.8%. This progression reflects the increasing model complexity and flexibility in capturing nonlinear relationships in the data.
                    </p>
                </div>
            </div>

            <div class="comparison-table">
                <table>
                    <tr>
                        <th>Method</th>
                        <th>Strengths</th>
                        <th>Limitations</th>
                        <th>Best Use Cases</th>
                    </tr>
                    <tr>
                        <td>Naive Bayes</td>
                        <td>Fast training, works well with limited data, probabilistic output</td>
                        <td>Independence assumption often violated, lower accuracy</td>
                        <td>Baseline model, real-time classification, text-based audio metadata</td>
                    </tr>
                    <tr>
                        <td>Decision Trees</td>
                        <td>Highly interpretable, handles mixed feature types, no scaling required</td>
                        <td>Prone to overfitting, unstable (small changes in data can cause large changes in tree)</td>
                        <td>Exploratory analysis, rule extraction, feature importance assessment</td>
                    </tr>
                    <tr>
                        <td>Logistic Regression</td>
                        <td>Well-calibrated probabilities, efficient for multi-class problems, interpretable weights</td>
                        <td>Limited to linear decision boundaries, sensitive to feature scaling</td>
                        <td>Binary audio classification, probability estimation, baseline model</td>
                    </tr>
                    <tr>
                        <td>SVM</td>
                        <td>Highest accuracy, effective in high dimensions, robust generalization</td>
                        <td>Computationally intensive, sensitive to hyperparameters, less interpretable</td>
                        <td>Optimal classification accuracy, complex audio patterns, black-box classification</td>
                    </tr>
                </table>
            </div>

            <p class="text-justified">
                Each method offers a unique balance of interpretability, performance, and computational efficiency. For audio classification, the most appropriate model depends on specific requirements:
            </p>
            <ul class="ul-list">
                <li>If computational efficiency and real-time classification are priorities, Naive Bayes provides a good baseline.</li>
                <li>If understanding feature relationships and extracting rules are important, Decision Trees offer the most transparency.</li>
                <li>If well-calibrated probability estimates are needed, Logistic Regression is ideal.</li>
                <li>If maximum classification accuracy is the goal and computational resources are available, SVMs with the RBF kernel deliver the best performance.</li>
            </ul>

            <p class="text-justified">
                In practice, ensemble methods that combine these approaches or deeper neural network architectures might achieve even better performance for complex audio classification tasks. However, the methods explored here provide strong foundations and valuable insights into the acoustic properties that differentiate various sound categories.
            </p>
        </div> -->

        <div class="navigation-buttons">
            <button class="nav-button" onclick="location.href='unsupervised.html'">
                ← Back to Unsupervised Learning
            </button>
            <button class="nav-button" onclick="location.href='conclusion.html'">
                Next: Conclusion →
            </button>
        </div>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/aos/2.3.4/aos.js"></script>
    <script>
        // Initialize AOS
        AOS.init();
        
        // Smooth scrolling for anchor links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function(e) {
                e.preventDefault();
                
                const targetId = this.getAttribute('href');
                const targetElement = document.querySelector(targetId);
                
                if (targetElement) {
                    window.scrollTo({
                        top: targetElement.offsetTop - 100, // Offset for fixed navbar
                        behavior: 'smooth'
                    });
                }
            });
        });
    </script>
</body>
</html>